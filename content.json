{"meta":{"title":"水果点心","subtitle":"beautiful & delicious","description":"不是一个关于吃的网站","author":"lukey","url":"https://luolizhi.github.io","root":"/"},"pages":[{"title":"","date":"2019-11-29T11:45:41.534Z","updated":"2019-11-29T11:45:41.534Z","comments":false,"path":"categories/index.html","permalink":"https://luolizhi.github.io/categories/index.html","excerpt":"","text":""},{"title":"about","date":"2019-08-05T02:22:43.000Z","updated":"2019-11-29T11:45:41.533Z","comments":true,"path":"about/index.html","permalink":"https://luolizhi.github.io/about/index.html","excerpt":"","text":"一枚软件工程师，专注docker，kubernetes，devops，golang等技术。"},{"title":"","date":"2019-11-29T11:45:41.534Z","updated":"2019-11-29T11:45:41.534Z","comments":true,"path":"custom/index.html","permalink":"https://luolizhi.github.io/custom/index.html","excerpt":"","text":"layout: pagetitle: customdescription: 用户自定义页面功能演示 date: 2019-08-05 10:27:22Image image Blockquote 当blockquote、img、pre、figure为第一级内容时，在page布局中拥有card阴影，所有标题居中展示。 Content@card{ 目前的想法是预定义一系列内容模块，通过像输入 Markdown 标记一样来简单调用。好在 Markdown 没有把所有便于输入的符号占用，最终我定义了@moduleName{ ... }这种标记格式。如果你使用过Asp.Net MVC，一定会很熟悉这种用法，没错，就是razor。 page布局中的title和subtitle对应 Markdown 中的title和description。 基本的内容容器还是card，你可以这样使用card： 12345@card&#123;在`page`页中，建议把内容都放到`card`中。&#125; 需要注意的是：标记与内容之间必须空一行隔开。至于为何要这样，看到最后就明白了。 } Column@column-2{ @card{ 左与card标记类似，分栏的标记是这样的： 123456789101112131415@column-2&#123;@card&#123;# 左&#125;@card&#123;# 右&#125;&#125; 为了移动端观感，当屏幕宽度小于 480 时，column将换行显示。 } @card{ 右column中的每一列具有等宽、等高的特点，最多支持三栏： 123456789101112131415161718192021@column-3&#123;@card&#123;左&#125;@card&#123;中&#125;@card&#123;右&#125;&#125; } } Three columns@column-3{ @card{ 话式片平九业影查类办细开被支，置军争里老5备才才目板。 且数置百容机，规的空界往，十陕志入。料解格清收权厂值动且习，识生能化路速年边，类儿2带杏性热求已。 } @card{ 话式片平九业影查类办细开被支，置军争里老5备才才目板。 且数置百容机，规的空界往，十陕志入。料解格清收权厂值动且习，识生能化路速年边，类儿2带杏性热求已。 } @card{ 话式片平九业影查类办细开被支，置军争里老5备才才目板。 且数置百容机，规的空界往，十陕志入。料解格清收权厂值动且习，识生能化路速年边，类儿2带杏性热求已。 } } Timeline@card{ 在timeline模块中，你的 5 号标题#####和六号标题######将被“征用”，用作时间线上的标记点： 123456789101112@timeline&#123;##### 2016@item&#123;###### 11月6日为 Card theme 添加 page layout。&#125;&#125; @item中多行内容可以换行输入，目前不允许隔行： 12345678910111213141516171819202122@timeline&#123;##### 2016@item&#123;###### 11月6日第一行 第二行 /* ok */&#125;@item&#123;###### 11月6日第一行第二行 /* error */&#125;&#125; } @timeline{ 2016@item{ 11月6日为 Card theme 添加 page layout。加快绿化空间好看 } @item{ 10月31日本地化多说。 } @item{ 10月24日为 Indigo 主题创建 Card 分支。 } 2015@item{ 2月24日发布 Indigo 主题到 hexo.io。 } @item{ 1月22日创建 Indigo 主题。 } } CodeBlock12345// 自定义内容块实现page.content.replace(/&lt;p&gt;&#125;&lt;\\/p&gt;/g, '&lt;/div&gt;') .replace(/&lt;p&gt;@([\\w-]+)&#123;&lt;\\/p&gt;/g, function(match, $1)&#123; return '&lt;div class=\"'+ $1 +'\"&gt;' &#125;) @card{ 这里可以解释，为什么标记之间必须要隔一行了。 当你在 Markdown 中隔行输入时，会形成新的段落，而如果一个段落中的内容仅仅是我们约定的标记，就可以用很容易的用正则匹配到替换为对应的模块容器。 } End@card{ 为了解决 Hexo 自定义页面slug为空不能很好的使用多说评论这个问题，现在已经给每个自定义页面自动生成了hexo-page-path这种格式的slug。本来准备用date做格式的最后一节，测试中发现 page 中的date值为修改时间，是动态的。综合考虑使用了路径path。 以后可以根据需要添加更多模块支持。 打赏和评论默认开启，可根据需要在 Markdown 头部定义是否关闭。 }"},{"title":"","date":"2019-11-29T11:45:41.540Z","updated":"2019-11-29T11:45:41.540Z","comments":false,"path":"tags/index.html","permalink":"https://luolizhi.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Operator SDK User Guide（翻译）","slug":"operator-user-guide-v0.12","date":"2019-11-30T02:29:38.000Z","updated":"2019-11-30T02:45:58.520Z","comments":true,"path":"2019/11/30/operator-user-guide-v0.12/","link":"","permalink":"https://luolizhi.github.io/2019/11/30/operator-user-guide-v0.12/","excerpt":"","text":"Operator SDK 用户指南 v0.12（翻译） 本指南介绍了使用 operator-sdk CLI 工具和控制器运行时库的 API 构建简单的 memcached-operator 的示例。 要了解如何使用 Ansible 或 Helm 创建 operator，查看 Ansible Operator 用户指南 或者 Helm Operator 用户指南。本文档的其余部分将展示如何在 Go 中编写 operator。 原文链接 先决条件 git go 版本 v1.12+. mercurial 版本 3.9+ docker 版本 17.03+. kubectl 版本 v1.11.3+. 可以访问版本 v1.11.3+ Kubernetes 集群。 注意：本指南使用 minikube 版本 v0.25.0+ 作为本地 Kubernetes 集群，同时使用 quay.io 作为公共镜像仓库。 安装 Operator SDK CLI请按照 安装指南 中的步骤进行操作，以了解如何安装 Operator SDK CLI 工具。 创建一个新项目使用 CLI 创建一个新的 memcached-operator 项目： 1234$ mkdir -p $HOME/projects$ cd $HOME/projects$ operator-sdk new memcached-operator --repo=github.com/example-inc/memcached-operator$ cd memcached-operator 要了解项目目录结构，请查看 项目布局 文档。 有关依赖管理的说明operator-sdk new 生成一个 go.mod 文件，该文件供 Go modules 使用。 当在 $GOPATH/src 路径之外创建项目时， 必须使用 --repo=&lt;path&gt; 参数，因为脚手架文件需要一个有效的模块路径。使用 SDK 之前，请确认开启支持 go module。从 Go modules Wiki 中： 你能通过以下两种方式之一开启支持 go module： 调用 go 命令的当前目录或其任何父目录中具有有效 go.mod 文件，并且未设置环境变量 GO111MODULE（或将其显式设置为 auto）。 使用 go 命令设置 GO111MODULE=on 环境变量。 Vendoring默认情况下 --vendor=false，因此 operator 的依赖项下载并缓存在 Go modules 缓存中。通过 operator-sdk 子命令对 go {build,clean,get,install,list,run,test} 的条用将会使用一个外部 modules 目录。执行 go help modules 获取更多信息。 项目初始化使用 --vendor=true 参数，Operator SDK 能够为 Go 项目依赖创建 vendor 文件夹。 Operator 范围阅读 operator scope 文档，了解如何在命令空间范围或者集群范围内运行你的operator。 1. 在集群中运行一个deploymet注意: 默认情况下，operator-sdk build 会调用 docker build，也可以选择调用 buildah bud。如果使用 buildah，请跳到下面的 operator-sdk build 调用说明。如果使用 docker，确认你的 docker 守护进程正在运行，并且可以在没有 sudo 的情况下运行 docker 客户端。您可以通过运行 docker version 来检查是否存在这种情况，该版本应该正确无误。 请根据你的 OS/distribution 上的说明，了解如何启动 docker 守护进程并根据需要配置访问权限。 注意: 如果存在 vendor/ 目录，运行下面命令： 1$ go mod vendor 在构建 memcached-operator 镜像之前。 构建 memcached-operator 镜像并推送到镜像仓库：123$ operator-sdk build quay.io/example/memcached-operator:v0.0.1$ sed -i 's|REPLACE_IMAGE|quay.io/example/memcached-operator:v0.0.1|g' deploy/operator.yaml$ docker push quay.io/example/memcached-operator:v0.0.1 验证 memcached-operator 已经启动并正在运行: 123$ kubectl get deploymentNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEmemcached-operator 1 1 1 1 1m 创建一个 Memcached CR示例 Memcached CR 会生成在 deploy/crds/cache.example.com_v1alpha1_memcached_cr.yaml 中：123456789$ cat deploy/crds/cache.example.com_v1alpha1_memcached_cr.yamlapiVersion: \"cache.example.com/v1alpha1\"kind: \"Memcached\"metadata: name: \"example-memcached\"spec: size: 3$ kubectl apply -f deploy/crds/cache.example.com_v1alpha1_memcached_cr.yaml 确认 memcached-operator 为 CR 创建了 deployment： 1234$ kubectl get deploymentNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEmemcached-operator 1 1 1 1 2mexample-memcached 3 3 3 3 1m 检查 pods 和 CR 的状态以确认其状态已经用 memcached pod 名称更新了： 123456$ kubectl get podsNAME READY STATUS RESTARTS AGEexample-memcached-6fd7c98d8-7dqdr 1/1 Running 0 1mexample-memcached-6fd7c98d8-g5k7v 1/1 Running 0 1mexample-memcached-6fd7c98d8-m7vn7 1/1 Running 0 1mmemcached-operator-7cc7cfdf86-vvjqk 1/1 Running 0 2m 12345678910111213141516171819$ kubectl get memcached/example-memcached -o yamlapiVersion: cache.example.com/v1alpha1kind: Memcachedmetadata: clusterName: \"\" creationTimestamp: 2018-03-31T22:51:08Z generation: 0 name: example-memcached namespace: default resourceVersion: \"245453\" selfLink: /apis/cache.example.com/v1alpha1/namespaces/default/memcacheds/example-memcached uid: 0026cc97-3536-11e8-bd83-0800274106a1spec: size: 3status: nodes: - example-memcached-6fd7c98d8-7dqdr - example-memcached-6fd7c98d8-g5k7v - example-memcached-6fd7c98d8-m7vn7 更新大小将 memcached CR 中的 spec.size 字段从3更改为4，并应用更改： 123456789$ cat deploy/crds/cache.example.com_v1alpha1_memcached_cr.yamlapiVersion: \"cache.example.com/v1alpha1\"kind: \"Memcached\"metadata: name: \"example-memcached\"spec: size: 4$ kubectl apply -f deploy/crds/cache.example.com_v1alpha1_memcached_cr.yaml 确认 operator 改变了 deployment 的大小： 123$ kubectl get deploymentNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEexample-memcached 4 4 4 4 5m 清理清理资源： 12345$ kubectl delete -f deploy/crds/cache.example.com_v1alpha1_memcached_cr.yaml$ kubectl delete -f deploy/operator.yaml$ kubectl delete -f deploy/role_binding.yaml$ kubectl delete -f deploy/role.yaml$ kubectl delete -f deploy/service_account.yaml 进阶主题增加第三方资源到你的 operator 中operator 的 Manager 支持在 client-go scheme 包中找到 Kubernetes 核心资源类型，并将你项目中 pkg/apis 目录下定义的所有自定义资源类型的 schemes 注册到 Kubernetes apiserver 中。12345678910import ( \"github.com/example-inc/memcached-operator/pkg/apis\" ...)// Setup Scheme for all resourcesif err := apis.AddToScheme(mgr.GetScheme()); err != nil &#123; log.Error(err, \"\") os.Exit(1)&#125; 要将第三方资源添加到 operator 中，你必须将其添加到 Manager’s scheme 中。通过创建一个 AddToScheme() 方法或者重用一个方法，你可以轻松地将资源加到你的 scheme 中。一个示例] 显示你定义一个函数，然后使用 runtime 包创建一个 SchemeBuilder。 注册 Manager 的 scheme为你的第三方资源调用 AddToScheme() 方法，并通过 mgr.GetScheme() 将其传递给 Manager’s scheme。 示例：1234567891011121314151617181920212223import ( .... routev1 \"github.com/openshift/api/route/v1\")func main() &#123; .... // Adding the routev1 if err := routev1.AddToScheme(mgr.GetScheme()); err != nil &#123; log.Error(err, \"\") os.Exit(1) &#125; .... // Setup all Controllers if err := controller.AddToManager(mgr); err != nil &#123; log.Error(err, \"\") os.Exit(1) &#125;&#125; 注意: 在将新的 import 路径添加到你的operator 项目中后，如果项目的根目录中存在 vendor/ 目录，请运行 go mod vendor 以满足这些依赖。 需要先添加您的第三方资源，然后才能在 &quot;Setup all Controllers&quot; 中添加控制器。 在删除的时候做些清理工作要实现复杂的删除逻辑，你可以在自定义资源中增加一个 finalizer。这将阻止你的自定义资源被删除，直到你删除 finalizer（例如，在清理逻辑成功运行之后）。有关更多信息，请参考official Kubernetes documentation on finalizers。 示例： 以下是 pkg/controller/memcached/memcached_controller.go 控制器文件的摘录。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697const memcachedFinalizer = \"finalizer.cache.example.com\"func (r *ReconcileMemcached) Reconcile(request reconcile.Request) (reconcile.Result, error) &#123; reqLogger := log.WithValues(\"Request.Namespace\", request.Namespace, \"Request.Name\", request.Name) reqLogger.Info(\"Reconciling Memcached\") // Fetch the Memcached instance memcached := &amp;cachev1alpha1.Memcached&#123;&#125; err := r.client.Get(context.TODO(), request.NamespacedName, memcached) if err != nil &#123; // If the resource is not found, that means all of // the finalizers have been removed, and the memcached // resource has been deleted, so there is nothing left // to do. if apierrors.IsNotFound(err) &#123; return reconcile.Result&#123;&#125;, nil &#125; return reconcile.Result&#123;&#125;, fmt.Errorf(\"could not fetch memcached instance: %s\", err) &#125; ... // Check if the Memcached instance is marked to be deleted, which is // indicated by the deletion timestamp being set. isMemcachedMarkedToBeDeleted := memcached.GetDeletionTimestamp() != nil if isMemcachedMarkedToBeDeleted &#123; if contains(memcached.GetFinalizers(), memcachedFinalizer) &#123; // Run finalization logic for memcachedFinalizer. If the // finalization logic fails, don't remove the finalizer so // that we can retry during the next reconciliation. if err := r.finalizeMemcached(reqLogger, memcached); err != nil &#123; return reconcile.Result&#123;&#125;, err &#125; // Remove memcachedFinalizer. Once all finalizers have been // removed, the object will be deleted. memcached.SetFinalizers(remove(memcached.GetFinalizers(), memcachedFinalizer)) err := r.client.Update(context.TODO(), memcached) if err != nil &#123; return reconcile.Result&#123;&#125;, err &#125; &#125; return reconcile.Result&#123;&#125;, nil &#125; // Add finalizer for this CR if !contains(memcached.GetFinalizers(), memcachedFinalizer) &#123; if err := r.addFinalizer(reqLogger, memcached); err != nil &#123; return reconcile.Result&#123;&#125;, err &#125; &#125; ... return reconcile.Result&#123;&#125;, nil&#125;func (r *ReconcileMemcached) finalizeMemcached(reqLogger logr.Logger, m *cachev1alpha1.Memcached) error &#123; // TODO(user): Add the cleanup steps that the operator // needs to do before the CR can be deleted. Examples // of finalizers include performing backups and deleting // resources that are not owned by this CR, like a PVC. reqLogger.Info(\"Successfully finalized memcached\") return nil&#125;func (r *ReconcileMemcached) addFinalizer(reqLogger logr.Logger, m *cachev1alpha1.Memcached) error &#123; reqLogger.Info(\"Adding Finalizer for the Memcached\") m.SetFinalizers(append(m.GetFinalizers(), memcachedFinalizer)) // Update CR err := r.client.Update(context.TODO(), m) if err != nil &#123; reqLogger.Error(err, \"Failed to update Memcached with finalizer\") return err &#125; return nil&#125;func contains(list []string, s string) bool &#123; for _, v := range list &#123; if v == s &#123; return true &#125; &#125; return false&#125;func remove(list []string, s string) []string &#123; for i, v := range list &#123; if v == s &#123; list = append(list[:i], list[i+1:]...) &#125; &#125; return list&#125; Metrics要了解 operator SDK 中 metrics 如何工作，请阅读用户文档 metrics section。 领导人选举在 operator 的生命周期内，例如在为 operator 升级时，在任何给定的时间可能有多个实例在运行。在这种情况下，有必要避免通过领导者选举在多个 operator 实例之间发生争用，以便只有一个领导者实例处理调和，而其他实例处于非活动状态，但准备好在领导者下台时接管。 有两种不同的领导者选举实现方式可供选择，每种实现方式都有其自身的取舍。 Leader-for-life：领导者 pod 仅在被删除时才放弃领导（通过垃圾回收）。此实现避免了 2 个实例错误地作为领导者运行（脑裂）的可能性。但是，此方法可能会导致选举新领导人的时间延迟。例如，当领导者 pod 在无响应或者在一个分区的节点上时，pod-eviction-timeout 参数指示从节点删除领导者 pod 时间并下台的时间（默认是5m）。 Leader-with-lease：领导者 pod 会定期续签领导租约，并在无法续签租约是放弃领导。这种实现方式可以更快的转移领导权，但是在 certain situations 可以会出现脑裂的情况。 默认情况下，operator SDK 使用 leader-for-life 的方式。但是，你应该参考上述两种方法的文档，以考虑对你的用例有意义的折衷方案。 下面的示例说明了如何使用这两个选项： Leader for life通过对 leader.Become() 方法的调用阻止 operator 重新成为领导，直到其创建名为 memcached-operator-lock 的 configmap。 1234567891011121314import ( ... \"github.com/operator-framework/operator-sdk/pkg/leader\")func main() &#123; ... err = leader.Become(context.TODO(), \"memcached-operator-lock\") if err != nil &#123; log.Error(err, \"Failed to retry for leader lock\") os.Exit(1) &#125; ...&#125; 如果 operator 不在集群中运行，则 leader.Become() 将返回无错误的消息以跳过领导人选举，因为它无法检测 operator 的命名空间。 Leader with lease可以通过 Manager Options 启用领导者租用方式进行领导人选举。 123456789101112131415import ( ... \"sigs.k8s.io/controller-runtime/pkg/manager\")func main() &#123; ... opts := manager.Options&#123; ... LeaderElection: true, LeaderElectionID: \"memcached-operator-lock\" &#125; mgr, err := manager.New(cfg, opts) ...&#125; 当 operator 不在集群中运行时，Manager 将在启动时返回错误，因为它无法检测到 operator 的名称空间以创建用于领导者选举的 configmap。 您可以通过设置 Manager 的 LeaderElectionNamespace 选项来覆盖此命名空间。","categories":[],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://luolizhi.github.io/tags/kubernetes/"},{"name":"operator","slug":"operator","permalink":"https://luolizhi.github.io/tags/operator/"}]},{"title":"API-gateway","slug":"API-gateway","date":"2019-08-27T01:25:34.000Z","updated":"2019-11-30T04:09:21.503Z","comments":true,"path":"2019/08/27/API-gateway/","link":"","permalink":"https://luolizhi.github.io/2019/08/27/API-gateway/","excerpt":"","text":"开源API网关比较 开源网关 简介 zuul Netflix 开源，基于 JVM 路由和服务端的负载均衡器。（已经停止维护） kong 基于 OpenResty（Nginx + Lua 模块）编写的高可用、易扩展的，由 Mashape 公司开源的 API Gateway 项目 。 spring cloud gateway Spring Cloud 团队的一个全新项目，基于 Spring 5.0、SpringBoot2.0、Project Reactor 等技术开发的网关， 目标是替代 Netflix Zuul 。 Traefik 一个现代 HTTP 反向代理和负载均衡器，可以轻松部署微服务，Traeffik 可以与您现有的组件（Docker、Swarm，Kubernetes，Marathon，Consul，Etcd，…）集成，并自动动态配置。 从开源社区活跃度来看，无疑是Kong和Traefik较好；从成熟度来看，较好的是Kong、Traefik；从性能角度来看，Kong要比其他几个领先一些；从架构优势的扩展性来看，Kong有丰富的插件，Ambassador也有插件但不多，而Zuul是完全需要自研，但Zuul由于与Spring Cloud深度集成，使用度也很高。 API 网关选型参考,原理讲解 API 网关出现的原因是微服务架构的出现，不同的微服务一般会有不同的网络地址，而外部客户端可能需要调用多个服务的接口才能完成一个业务需求，如果让客户端直接与各个微服务通信，会有以下的问题： 客户端会多次请求不同的微服务，增加了客户端的复杂性。 存在跨域请求，在一定场景下处理相对复杂。 认证复杂，每个服务都需要独立认证。 难以重构，随着项目的迭代，可能需要重新划分微服务。例如，可能将多个服务合并成一个或者将一个服务拆分成多个。如果客户端直接与微服务通信，那么重构将会很难实施。 某些微服务可能使用了防火墙 / 浏览器不友好的协议，直接访问会有一定的困难。","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"https://luolizhi.github.io/tags/微服务/"},{"name":"gateway","slug":"gateway","permalink":"https://luolizhi.github.io/tags/gateway/"}]},{"title":"helm 安装 logstash 同步mysql数据到elasticsearch","slug":"helm 安装 logstash","date":"2019-08-25T05:25:34.000Z","updated":"2019-11-30T04:00:18.583Z","comments":true,"path":"2019/08/25/helm 安装 logstash/","link":"","permalink":"https://luolizhi.github.io/2019/08/25/helm 安装 logstash/","excerpt":"","text":"安装helm安装到kubernetes集群参考地址：https://github.com/helm/charts/tree/master/stable/logstash 配置文件详解logstash-input-jdbc使用 logstash-input-jdbc 插件读取 mysql 的数据，这个插件的工作原理比较简单，就是定时执行一个 sql，然后将 sql 执行的结果写入到流中，增量获取的方式没有通过 binlog 方式同步，而是用一个递增字段作为条件去查询，每次都记录当前查询的位置，由于递增的特性，只需要查询比当前大的记录即可获取这段时间内的全部增量，一般的递增字段有两种，AUTO_INCREMENT 的主键 id 和 ON UPDATE CURRENT_TIMESTAMP 的 update_time 字段，id 字段只适用于那种只有插入没有更新的表，update_time 更加通用一些，建议在 mysql 表设计的时候都增加一个 update_time 字段 123456789101112131415input &#123; jdbc &#123; jdbc_driver_library =&gt; \"../mysql-connector-java-5.1.38.jar\" jdbc_driver_class =&gt; \"com.mysql.jdbc.Driver\" jdbc_connection_string =&gt; \"jdbc:mysql://&lt;mysql_host&gt;:3306/woa\" jdbc_user =&gt; \"&lt;username&gt;\" jdbc_password =&gt; \"&lt;password&gt;\" schedule =&gt; \"* * * * *\" statement =&gt; \"SELECT * FROM table WHERE update_time &gt;= :sql_last_value\" use_column_value =&gt; true tracking_column_type =&gt; \"timestamp\" tracking_column =&gt; \"update_time\" last_run_metadata_path =&gt; \"syncpoint_table\" &#125;&#125; jdbc_driver_library: jdbc mysql 驱动的路径 jdbc_driver_class: 驱动类的名字，mysql 填 com.mysql.jdbc.Driver 就好了 jdbc_connection_string: mysql 地址 jdbc_user: mysql 用户 jdbc_password: mysql 密码 schedule: 执行 sql 时机，类似 crontab 的调度 statement: 要执行的 sql，以 “:” 开头是定义的变量，可以通过 parameters 来设置变量，这里的 sql_last_value 是内置的变量，表示上一次 sql 执行中 update_time 的值，这里 - update_time 条件是 &gt;= 因为时间有可能相等，没有等号可能会漏掉一些增量 use_column_value: 使用递增列的值 tracking_column_type: 递增字段的类型，numeric 表示数值类型, timestamp 表示时间戳类型 tracking_column: 递增字段的名称，这里使用 update_time 这一列，这列的类型是 timestamp last_run_metadata_path: 同步点文件，这个文件记录了上次的同步点，重启时会读取这个文件，这个文件可以手动修改 logstash-output-elasticsearch123456789output &#123; elasticsearch &#123; hosts =&gt; [\"127.0.0.1\"] user =&gt; \"&lt;user&gt;\" password =&gt; \"&lt;password&gt;\" index =&gt; \"table\" document_id =&gt; \"%&#123;id&#125;\" &#125;&#125; hosts: es 集群地址 user: es 用户名 password: es 密码 index: 导入到 es 中的 index 名，这里我直接设置成了 mysql 表的名字 document_id: 导入到 es 中的文档 id，这个需要设置成主键，否则同一条记录更新后在 es 中会出现两条记录，%{id} 表示引用 mysql 表中 id 字段的值 定制jdbc镜像123FROM docker.elastic.co/logstash/logstash-oss:7.1.1RUN logstash-plugin install logstash-input-jdbcADD mysql-connector-java-5.1.38.jar /usr/share/logstash/mysql-connector-java-5.1.38.jar 镜像docker.elastic.co/logstash/logstash-oss:7.1.1的工作目录为/usr/share/logstash/ 所以其他配置可以以此为当前目录配置相对路径。 自定义value值123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136service: type: NodePort## Custom files that can be referenced by plugins.## Each YAML heredoc will become located in the logstash home directory under## the files subdirectory.## 默认会在/usr/share/logstash/files/目录下生成单独的文件，供后面output自定义template使用,可以定义多个template文件。文件格式请严格参考another-template.json,特别是properties字段。files: # logstash-template.json: |- # &#123; # \"order\": 0, # \"version\": 1, # \"index_patterns\": [ # \"logstash-*\" # ], # \"settings\": &#123; # \"index\": &#123; # \"refresh_interval\": \"5s\" # &#125; # &#125;, # \"mappings\": &#123; # \"doc\": &#123; # \"_meta\": &#123; # \"version\": \"1.0.0\" # &#125;, # \"enabled\": false # &#125; # &#125;, # \"aliases\": &#123;&#125; # &#125; # another-template.json: |- # &#123; # \"order\": 0, # \"version\": 1, # \"index_patterns\": [ # \"test*\" # ], # \"settings\": &#123; # \"index\": &#123; # \"refresh_interval\": \"5s\" # &#125; # &#125;, # \"mappings\": &#123; # \"_doc\": &#123; # \"_source\": &#123; # \"enabled\": false # &#125;, # \"properties\": &#123; # \"host_name\": &#123; # \"type\": \"keyword\" # &#125;, # \"created_at\": &#123; # \"type\": \"date\", # \"format\": \"EEE MMM dd HH:mm:ss Z yyyy\" # &#125; # &#125; # &#125; # &#125;, # \"aliases\": &#123;&#125; # &#125; ## NOTE: To achieve multiple pipelines with this chart, current best practice## is to maintain one pipeline per chart release. In this way configuration is## simplified and pipelines are more isolated from one another.inputs: main: |- input &#123; jdbc &#123; jdbc_connection_string =&gt; \"jdbc:mysql://&lt;host&gt;:3306/&lt;database&gt;\" jdbc_user =&gt; \"&lt;user_name&gt;\" jdbc_password =&gt; \"&lt;password&gt;\" jdbc_driver_library =&gt; \"/usr/share/logstash/mysql-connector-java-5.1.38.jar\" #全路径 # jdbc_driver_library =&gt; \"./mysql-connector-java-5.1.38.jar\" # 也可以使用相对路径，默认路径为/usr/share/logstash/ jdbc_driver_class =&gt; \"com.mysql.jdbc.Driver\" jdbc_paging_enabled =&gt; \"true\" schedule =&gt; \"* * * * *\" statement =&gt; \"SELECT * from user\" type =&gt; \"helloworld\" &#125; # udp &#123; # port =&gt; 1514 # type =&gt; syslog # &#125; # tcp &#123; # port =&gt; 1514 # type =&gt; syslog # &#125; beats &#123; port =&gt; 5044 &#125; # http &#123; # port =&gt; 8080 # &#125; # kafka &#123; # ## ref: https://www.elastic.co/guide/en/logstash/current/plugins-inputs-kafka.html # bootstrap_servers =&gt; \"kafka-input:9092\" # codec =&gt; json &#123; charset =&gt; \"UTF-8\" &#125; # consumer_threads =&gt; 1 # topics =&gt; [\"source\"] # type =&gt; \"example\" # &#125; &#125;filters: # main: |- # filter &#123; # &#125;outputs: main: |- output &#123; # stdout &#123; codec =&gt; rubydebug &#125; elasticsearch &#123; hosts =&gt; [\"$&#123;ELASTICSEARCH_HOST&#125;:$&#123;ELASTICSEARCH_PORT&#125;\"] manage_template =&gt; false index =&gt; \"%&#123;[@metadata][beat]&#125;-%&#123;+YYYY.MM.dd&#125;\" &#125; # kafka &#123; # ## ref: https://www.elastic.co/guide/en/logstash/current/plugins-outputs-kafka.html # bootstrap_servers =&gt; \"kafka-output:9092\" # codec =&gt; json &#123; charset =&gt; \"UTF-8\" &#125; # compression_type =&gt; \"lz4\" # topic_id =&gt; \"destination\" # &#125; elasticsearch &#123; hosts =&gt; [\"$&#123;ELASTICSEARCH_HOST&#125;:$&#123;ELASTICSEARCH_PORT&#125;\"] # index名 index =&gt; \"hello\" # 需要关联的数据库中有有一个id字段，对应索引的id号 manage_template =&gt; true template =&gt;\"/usr/share/logstash/files/logstash-template.json\" #也可以使用相对路径\"./files/logstash-template.json\" template_overwrite =&gt;true template_name =&gt; \"helloword\" &#125; &#125;","categories":[],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://luolizhi.github.io/tags/kubernetes/"},{"name":"logstash","slug":"logstash","permalink":"https://luolizhi.github.io/tags/logstash/"}]},{"title":"kubesphere","slug":"kubesphere","date":"2019-08-23T02:20:27.000Z","updated":"2019-11-29T11:45:41.531Z","comments":true,"path":"2019/08/23/kubesphere/","link":"","permalink":"https://luolizhi.github.io/2019/08/23/kubesphere/","excerpt":"","text":"KubeSphere 安装(all-in-one)KubeSphere 帮助企业在云、虚拟化及物理机等任何环境中快速构建、部署及运维容器架构，轻松实现服务治理、 DevOps 与 CI/CD 、应用管理、大数据、人工智能、IAM 以及监控日志等业务场景。 环境准备安装参考 centos7.6 8核16G，100G系统盘 准备安装包在线版（2.0.2）curl -L https://kubesphere.io/download/stable/advanced-2.0.2 &gt; advanced-2.0.2.tar.gz &amp;&amp; tar -zxf advanced-2.0.2.tar.gz &amp;&amp; cd kubesphere-all-advanced-2.0.2/scripts 安装 KubeSphere 建议使用 root 用户安装，执行 install.sh 脚本： $ ./install.sh 输入数字 1 选择第一种即 all-in-one 模式开始安装： 12345678910################################################ KubeSphere Installer Menu################################################* 1) All-in-one* 2) Multi-node* 3) Quit################################################https://kubesphere.io/ 2018-07-08################################################Please input an option: 1 测试 KubeSphere 单节点安装是否成功： (1) 待安装脚本执行完后，当看到如下 “Successful” 界面，则说明 KubeSphere 安装成功。 1234567891011successsful!######################################################## Welcome to KubeSphere! ########################################################Console: http://192.168.0.8:30880Account: adminPassword: P@88w0rdNOTE：Please modify the default password after login.##################################################### kubesphere 安装（在现有k8s集群上）安装脚本地址：https://github.com/kubesphere/ks-installer 一直有helm报错未能成功安装。 安装过程的问题 openldap 这个组件启动报错，因为 ks-account 组件又是依赖 openldap 这个组件的，所以同样启动报错，在安装过程中 openldap 出现了类似如下错误信息。 123456rm: cannot remove ‘/container/service/slapd/assets/config/bootstrap/ldif/readonly-user’: Directory not empty rm: cannot remove ‘/container/service/slapd/assets/config/bootstrap/schema/mmc’: Directory not empty rm: cannot remove ‘/container/service/slapd/assets/config/replication’: Directory not empty rm: cannot remove ‘/container/service/slapd/assets/config/tls’: Directory not empty *** /container/run/startup/slapd failed with status 1 解决方法： 修改kubesphere/roles/prepare/base/templates/ks-account-init.yaml.j2文件，在 openldap 这个 Deployment 下面容器中添加启动参数–copy-service 12345678910111213141516#vim kubesphere/roles/prepare/base/templates/ks-account-init.yaml.j2 +122 spec: containers: - env: - name: LDAP_ORGANISATION value: kubesphere - name: LDAP_DOMAIN value: kubesphere.io - name: LDAP_ADMIN_PASSWORD value: admin image: &#123;&#123; openldap_repo &#125;&#125;:&#123;&#123; openldap_tag &#125;&#125; imagePullPolicy: IfNotPresent args: # 添加该启动参数 - --copy-service name: openldap sonarqube安装报错 解决方法：不安装sonarqube，修改conf/vars.yml文件，将true改为false 1234#vim conf/vars.yml +210## sonarqubesonarqube_enable: false #改为false","categories":[],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://luolizhi.github.io/tags/kubernetes/"},{"name":"kubesphere","slug":"kubesphere","permalink":"https://luolizhi.github.io/tags/kubesphere/"}]},{"title":"elasticsearch 单机 docker 集群","slug":"elasticsearch 单机 docker 集群","date":"2019-08-15T01:19:04.000Z","updated":"2019-11-30T04:10:35.376Z","comments":true,"path":"2019/08/15/elasticsearch 单机 docker 集群/","link":"","permalink":"https://luolizhi.github.io/2019/08/15/elasticsearch 单机 docker 集群/","excerpt":"","text":"单机docker启动 elasticsearch 集群 安装版本 elasticsearch 6.7 参考网址, kibana 6.7 参考网址 注意镜像名称是elasticsearch-oss:6.7.0，这个-oss表示不包括X-Pack的ES镜像，这也是在6.0+版本后划分的，剩下两种类型是basic(默认)和platinum，具体官方说明可以看下图。 开发环境docker run -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; docker.elastic.co/elasticsearch/elasticsearch:6.7.2 生产环境The vm.max_map_count setting should be set permanently in /etc/sysctl.conf: 12345$ grep vm.max_map_count /etc/sysctl.confvm.max_map_count=262144#为空需要修改echo \"vm.max_map_count=262144\" &gt; /etc/sysctl.confsysctl -p 启动docker-compose up 停止docker-compose down -v 123456789101112131415161718192021222324252627282930313233343536373839404142434445#docker-compose.yamlversion: '2.2'services: elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:6.7.2 #可以换成私有镜像 container_name: elasticsearch environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - esdata1:/usr/share/elasticsearch/data ports: - 9200:9200 networks: - esnet elasticsearch2: image: docker.elastic.co/elasticsearch/elasticsearch:6.7.2 container_name: elasticsearch2 environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" - \"discovery.zen.ping.unicast.hosts=elasticsearch\" ulimits: memlock: soft: -1 hard: -1 volumes: - esdata2:/usr/share/elasticsearch/data networks: - esnetvolumes: esdata1: driver: local esdata2: driver: localnetworks: esnet: 测试集群12curl http://127.0.0.1:9200/_cat/health1472225929 15:38:49 docker-cluster green 2 2 4 2 0 0 0 0 - 100.0% 插件支持ik analysisIK是国内用得比较多的中文分词器，与ES安装集成也比较简单，首先进入dockerdocker exec -it elasticsearch bash，然后用命令./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.3.2/elasticsearch-analysis-ik-6.3.2.zip安装即可(需对应es版本)，安装完使用docker restart elasticsearch重启服务即可。IK支持两种分词方式，ik_smart和ik_max_word，前者分词粒度没有后者细，可以针对实际情况进行选择。 head pluginelasticsearch-head插件也是测试的时候用得比较多的插件，以前用ES2的时候是借助plugin脚本安装的，但这种方式在ES5.0之后被废弃了，然后作者也推荐了好几种方式，可以借助npm运行该服务，或者用docker运行服务，不过为了简单起见我最后选的是Chrome extension这种方式。 kibana 安装 启动docker-compose up 停止docker-compose down -v 12345678#docker-compose.yamlversion: '2'services: kibana: image: docker.elastic.co/kibana/kibana:6.7.2 #修改为自定义镜像 environment: SERVER_NAME: kibana.example.org #kibana server.name的值 ELASTICSEARCH_HOSTS: http://elasticsearch.example.org #修改为es集群的地址 单个docker-compose.yaml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556version: '2.2'services: elasticsearch: image: harbor.cty.com/library/elasticsearch-oss-ik-config:6.7.0 container_name: elasticsearch1 environment: - cluster.name=elasticsearch - node.name=node1 - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - esdata1:/usr/share/elasticsearch/data ports: - 9200:9200 - 9300:9300 networks: - esnet elasticsearch2: image: harbor.cty.com/library/elasticsearch-oss-ik-config:6.7.0 container_name: elasticsearch2 environment: - cluster.name=elasticsearch - node.name=node2 - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" - \"discovery.zen.ping.unicast.hosts=elasticsearch\" ulimits: memlock: soft: -1 hard: -1 volumes: - esdata2:/usr/share/elasticsearch/data networks: - esnet kibana: image: harbor.cty.com/library/kibana-oss:6.7.0 environment: - \"SERVER_NAME=kibana\" - \"ELASTICSEARCH_HOSTS=http://192.168.1.150:9200\" #ip地址不能换 ports: - \"5601:5601\" depends_on: - elasticsearch - elasticsearch2volumes: #路径默认在/var/lib/docker/volumes/ esdata1: driver: local esdata2: driver: localnetworks: esnet:","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://luolizhi.github.io/tags/docker/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://luolizhi.github.io/tags/elasticsearch/"}]},{"title":"dockerfile 中 cmd 和 entrypoint 用法","slug":"Dockerfile中cmd和entrypoint","date":"2019-08-11T01:19:04.000Z","updated":"2019-11-30T03:56:57.476Z","comments":true,"path":"2019/08/11/Dockerfile中cmd和entrypoint/","link":"","permalink":"https://luolizhi.github.io/2019/08/11/Dockerfile中cmd和entrypoint/","excerpt":"","text":"entrypoint cmd在docker run image cmd arg1时，总是相当于CMD的[“cmd”, “arg1”]形式。 此外，docker-compose.yml的情况与docker run类似。 Dockerfile中cmd命令的三种形式The CMD instruction has three forms: CMD [“executable”,”param1”,”param2”] (exec form, this is the preferred form) CMD [“param1”,”param2”] (as default parameters to ENTRYPOINT) CMD command param1 param2 (shell form) 第一种(推荐)采用中括号形式，那么第一个参数必须是命令的全路径才行。而且，一个dockerfile至多只能有一个cmd，如果有多个，只有最后一个生效。 123FROM ubuntuCMD [\"/bin/bash\", \"-c\", \"echo 'hello cmd!'\"] 第二种是作为参数传递给entrypoint第三种shell form，即没有中括号的形式。那么命令command默认是在”/bin/sh -c”下执行的。(等价于第一种的写法) 123FROM ubuntuCMD echo \"hello cmd!\" ENTRYPOINT has two forms: ENTRYPOINT [“executable”, “param1”, “param2”] (exec form, preferred) ENTRYPOINT command param1 param2 (shell form) 分为命令行和shell两种。先看命令行模式，也就是带中括号的。和cmd的中括号形式是一致的，但是这里貌似是在shell的环境下执行的，与cmd有区别。如果run命令后面有东西，那么后面的全部都会作为entrypoint的参数。如果docker run后面没有额外的东西，但是cmd有，那么cmd的全部内容会作为entrypoint的参数，这同时是cmd的第二种用法。这也是网上说的entrypoint不会被覆盖。当然如果要在run里面覆盖，也是有办法的，使用–entrypoint即可。 docker-compose Dockerfile Kubernetes中CMD和entrypoint区别 Description Docker field name Docker-compose Kubernetes field name The command run by the container Entrypoint Entrypoint command The arguments passed to the command Cmd command args 如果要覆盖默认的Entrypoint 与 Cmd，需要遵循如下规则：(Docker 与 Kubernetes, k8s中的设置会覆盖容器默认的命令) 如果在容器配置中没有设置command 或者 args，那么将使用Docker镜像自带的命令及其入参。 如果在容器配置中只设置了command但是没有设置args,那么容器启动时只会执行该命令，Docker镜像中自带的命令及其入参会被忽略。 如果在容器配置中只设置了args,那么Docker镜像中自带的命令会使用该新入参作为其执行时的入参。 如果在容器配置中同时设置了command 与 args，那么Docker镜像中自带的命令及其入参会被忽略。容器启动时只会执行配置中设置的命令，并使用配置中设置的入参作为命令的入参。 下表涵盖了各类设置场景： Image Entrypoint Image Cmd Container command Container args Command run [/ep-1] [foo bar] [ep-1 foo bar] [/ep-1] [foo bar] [/ep-2] [ep-2] [/ep-1] [foo bar] [zoo boo] [ep-1 zoo boo] [/ep-1] [foo bar] [/ep-2] [zoo boo] [ep-2 zoo boo] 空白表示没设置该参数 12345env:- name: MESSAGE value: \"hello world\"command: [\"/bin/echo\"]args: [\"$(MESSAGE)\"]","categories":[],"tags":[{"name":"dockerfile","slug":"dockerfile","permalink":"https://luolizhi.github.io/tags/dockerfile/"},{"name":"kubernetes","slug":"kubernetes","permalink":"https://luolizhi.github.io/tags/kubernetes/"}]},{"title":"cat安装使用","slug":"cat安装使用","date":"2019-08-07T05:29:38.000Z","updated":"2019-11-29T11:45:41.524Z","comments":true,"path":"2019/08/07/cat安装使用/","link":"","permalink":"https://luolizhi.github.io/2019/08/07/cat安装使用/","excerpt":"","text":"cat 安装步骤服务端部署官方地址：https://github.com/dianping/cat/wiki/readme_server 客户端集成官方地址：https://github.com/dianping/cat/wiki/readme_client","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"https://luolizhi.github.io/tags/微服务/"},{"name":"cat","slug":"cat","permalink":"https://luolizhi.github.io/tags/cat/"}]},{"title":"gogland查看kubernetes源码","slug":"配置goLand查看k8s源码","date":"2019-08-05T01:15:02.000Z","updated":"2019-11-30T04:12:27.750Z","comments":true,"path":"2019/08/05/配置goLand查看k8s源码/","link":"","permalink":"https://luolizhi.github.io/2019/08/05/配置goLand查看k8s源码/","excerpt":"","text":"gogland配置读取kubernetes代码 先下载kubernetes代码 下载代码并解压出来并按照src\\k8s.io\\kubernetes新建目录，最后一级为下载的源码目录。 接下来利用goland打开代码，到src一级。 设置projectpath；添加下面三行C:\\Users\\lukey\\Documents\\code\\go\\k8s\\src\\k8s.io\\kubernetes\\vendorC:\\Users\\lukey\\Documents\\code\\go\\k8s\\C:\\Users\\lukey\\Documents\\code\\go\\k8s\\src\\k8s.io\\kubernetes\\staging\\src 过一会，代码中的import就由红色变成绿色，就可以直接ctrl+点击到声明了。","categories":[],"tags":[{"name":"kuberenetes","slug":"kuberenetes","permalink":"https://luolizhi.github.io/tags/kuberenetes/"}]},{"title":"prometheus部署","slug":"kubernetes部署Prometheus，配置grafana","date":"2018-12-01T10:29:38.000Z","updated":"2019-11-30T02:43:51.882Z","comments":true,"path":"2018/12/01/kubernetes部署Prometheus，配置grafana/","link":"","permalink":"https://luolizhi.github.io/2018/12/01/kubernetes部署Prometheus，配置grafana/","excerpt":"","text":"kubernetes部署Prometheus 注意事项安装参考https://blog.qikqiak.com/post/kubernetes-monitor-prometheus-grafana/ 需要在grafana中增加Prometheus为datasource配置报警监控先在alerting中配置channel，比如钉钉，email等（）grafana4.4.3集成了钉钉。然后在配置的dashboard的graph中增加规则，触发告警系统。","categories":[],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://luolizhi.github.io/tags/k8s/"},{"name":"prometheus","slug":"prometheus","permalink":"https://luolizhi.github.io/tags/prometheus/"},{"name":"grafana","slug":"grafana","permalink":"https://luolizhi.github.io/tags/grafana/"}]},{"title":"ubuntu16.04 搭建nfs服务器","slug":"ubuntu16.04搭建nfs服务器","date":"2018-12-01T02:50:27.000Z","updated":"2019-11-30T03:45:50.270Z","comments":true,"path":"2018/12/01/ubuntu16.04搭建nfs服务器/","link":"","permalink":"https://luolizhi.github.io/2018/12/01/ubuntu16.04搭建nfs服务器/","excerpt":"","text":"ubuntu16.04 搭建nfs服务器一、服务器端：1.1安装NFS服务：#执行以下命令安装NFS服务器， #apt会自动安装nfs-common、rpcbind等13个软件包 sudo apt install nfs-kernel-server 1.2编写配置文件：#编辑/etc/exports 文件： sudo vi /etc/exports #/etc/exports文件的内容如下：123/tmp *(rw,sync,no_subtree_check,no_root_squash)/data *(rw,sync,no_subtree_check,no_root_squash)/logs *(rw,sync,no_subtree_check,no_root_squash) 1.3创建共享目录#在服务器端创建/tmp /data和/logs共享目录123sudo mkdir -p /tmpsudo mkdir -p /datasudo mkdir -p /logs 1.4重启nfs服务：sudo service nfs-kernel-server restart 1.5常用命令工具：#在安装NFS服务器时，已包含常用的命令行工具，无需额外安装。 #显示已经mount到本机nfs目录的客户端机器。 sudo showmount -e localhost #将配置文件中的目录全部重新export一次！无需重启服务。 sudo exportfs -rv #查看NFS的运行状态 sudo nfsstat #查看rpc执行信息，可以用于检测rpc运行情况 sudo rpcinfo` #查看网络端口，NFS默认是使用111端口。 sudo netstat -tu -4 二、客户端：2.1安装客户端工具：#在需要连接到NFS服务器的客户端机器上， #需要执行以下命令，安装nfs-common软件包。 #apt会自动安装nfs-common、rpcbind等12个软件包 sudo apt install nfs-common 2.2查看NFS服务器上的共享目录#显示指定的（192.168.3.167）NFS服务器上export出来的目录 sudo showmount -e 192.168.3.167 2.3创建本地挂载目录sudo mkdir -p /mnt/data sudo mkdir -p /mnt/logs 2.4挂载共享目录#将NFS服务器192.168.3.167上的目录，挂载到本地的/mnt/目录下 sudo mount -t nfs 192.168.3.167:/data /mnt/data sudo mount -t nfs 192.168.3.167:/logs /mnt/logs #注：在没有安装nfs-common或者nfs-kernel-server软件包的机器上， #直接执行showmount、exportfs、nfsstat、rpcinfo等命令时， #系统会给出友好的提示， #比如直接showmount会提示需要执行sudo apt install nfs-common命令， #比如直接rpcinfo会提示需要执行sudo apt install rpcbind命令。 2.5卸载文件系统命令umount /mnt/dataumount /mnt/logs","categories":[],"tags":[{"name":"nfs","slug":"nfs","permalink":"https://luolizhi.github.io/tags/nfs/"}]},{"title":"kubernetes glusterfs","slug":"kubernetes-glusterfs","date":"2018-12-01T02:50:27.000Z","updated":"2019-11-30T03:58:20.103Z","comments":true,"path":"2018/12/01/kubernetes-glusterfs/","link":"","permalink":"https://luolizhi.github.io/2018/12/01/kubernetes-glusterfs/","excerpt":"","text":"kubernetes glusterfs安装 glusterfs12345# 先安装 gluster 源yum install centos-release-gluster -y# 安装 glusterfs 组件yum install -y glusterfs glusterfs-server glusterfs-fuse glusterfs-rdma glusterfs-geo-replication glusterfs-devel 123456## 创建 glusterfs 目录mkdir /opt/glusterd## 修改 glusterd 目录sed -i &apos;s/var\\/lib/opt/g&apos; /etc/glusterfs/glusterd.vol 12345678# 启动 glusterfssystemctl start glusterd.service# 设置开机启动systemctl enable glusterd.service#查看状态systemctl status glusterd.service 配置 glusterfs12345678# 配置 hostsvi /etc/hostsgluster-1 10.6.0.52gluster-2 10.6.0.53gluster-3 10.6.0.55gluster-4 10.6.0.56 123# 开放端口iptables -I INPUT -p tcp --dport 24007 -j ACCEPT 123# 创建存储目录mkdir /opt/gfs_data 1234567891011# 添加节点到 集群# 执行操作的本机不需要probe 本机[root@gluster-1 ~]#gluster peer probe gluster-2gluster peer probe gluster-3gluster peer probe gluster-4# 查看集群状态gluster peer status 配置 volume123456789101112# 创建 分布卷gluster volume create dht-volume transport tcp gluster-1:/opt/gfs_data gluster-2:/opt/gfs_data# 查看volume状态gluster volume info...Type: Distribute...# 启动 分布卷gluster volume start dht-volume 1234567891011121314# 创建 复制卷gluster volume create afr-volume replica 2 transport tcp gluster-1:/opt/afr_data gluster-2:/opt/afr_data# 查看volume状态gluster volume info...Type: Replicate...# 启动 复制卷gluster volume start afr-volume 123456789101112# 创建 条带卷gluster volume create str-volume stripe 2 transport tcp gluster-1:/opt/str_data gluster-2:/opt/str_data# 查看volume状态gluster volume info...Type: Stripe...# 启动 条带卷gluster volume start str-volume 123456789101112131415161718192021222324252627282930# 上面三种基本模式，可以互相组合# 这里我们使用 组合 分布式复制卷 需要最少4台服务器 replica 必须为倍数gluster volume create k8s-volume replica 2 transport tcp gluster-1:/opt/gfs_data gluster-2:/opt/gfs_data gluster-3:/opt/gfs_data gluster-4:/opt/gfs_data# 查看 volume 状态gluster volume infoVolume Name: k8s-volumeType: Distributed-ReplicateVolume ID: 981c41fa-bbe1-4a36-a1e2-9f76de1dc8f1Status: CreatedSnapshot Count: 0Number of Bricks: 2 x 2 = 4Transport-type: tcpBricks:Brick1: gluster-1:/opt/gfs_dataBrick2: gluster-2:/opt/gfs_dataBrick3: gluster-3:/opt/gfs_dataBrick4: gluster-4:/opt/gfs_dataOptions Reconfigured:transport.address-family: inetnfs.disable: on# 启动 k8s-volumegluster volume start k8s-volume gluster 调优1234567891011121314151617181920212223# 开启 指定 volume 的配额gluster volume quota k8s-volume enable# 限制 指定 volume 的配额gluster volume quota k8s-volume limit-usage / 5TB# 设置 cache 大小, 默认32MBgluster volume set k8s-volume performance.cache-size 4GB# 设置 io 线程, 太大会导致进程崩溃gluster volume set k8s-volume performance.io-thread-count 16# 设置 网络检测时间, 默认42sgluster volume set k8s-volume network.ping-timeout 10# 设置 目录索引的自动愈合进程gluster volume set k8s-volume cluster.self-heal-daemon on# 设置 自动愈合的检测间隔, 默认600sgluster volume set k8s-volume cluster.heal-timeout 300# 设置 写缓冲区的大小, 默认1Mgluster volume set k8s-volume performance.write-behind-window-size 1024MB kubernetes 配置 glusterfs1官方的文档 https://github.com/kubernetes/kubernetes/tree/master/examples/volumes/glusterfs kubernetes 安装客户端123456789101112# 在所有 k8s node 中安装 glusterfs 客户端yum install -y glusterfs glusterfs-fuse# 配置 hostsvi /etc/hostsgluster-1 10.6.0.52gluster-2 10.6.0.53gluster-3 10.6.0.55gluster-4 10.6.0.56 配置 endpoints123456789101112131415161718192021222324252627curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/examples/volumes/glusterfs/glusterfs-endpoints.json# 修改 endpoints.json ，配置 glusters 集群节点ip# 每一个 addresses 为一个 ip 组 &#123; &quot;addresses&quot;: [ &#123; &quot;ip&quot;: &quot;10.6.0.52&quot; &#125; ], &quot;ports&quot;: [ &#123; &quot;port&quot;: 1 &#125; ] &#125;,# 导入 glusterfs-endpoints.jsonkubectl apply -f glusterfs-endpoints.json# 查看 endpoints 信息kubectl get ep 配置 service123456789curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/examples/volumes/glusterfs/glusterfs-service.json# service.json 不需要配置，里面查找的是 enpointes 的名称与端口，端口默认配置为 1# 导入 glusterfs-service.jsonkubectl apply -f glusterfs-service.json# 查看 service 信息kubectl get svc 创建 测试 pod1234567891011121314151617181920212223curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/examples/volumes/glusterfs/glusterfs-pod.json# 编辑 glusterfs-pod.json# 修改 volumes 下的 path 为上面创建的 volume 名称&quot;path&quot;: &quot;k8s-volume&quot;# 导入 glusterfs-pod.jsonkubectl apply -f glusterfs-pod.json# 查看 pods 状态kubectl get pods NAME READY STATUS RESTARTS AGEglusterfs 1/1 Running 0 1m# 查看 pods 所在 nodekubectl describe pods/glusterfs# 登陆 node 物理机 使用 df 可查看 挂载目录 配置 pv PersistentVolume（pv）和 PersistentVolumeClaim（pvc）是k8s提供的两种API资源，用于抽象存储细节。管理员关注于如何通过pv提供存储功能而无需关注用户如何使用，同样的用户只需要挂载pvc到容器中而不需要关注存储卷采用何种技术实现。 pvc和pv的关系与pod和node关系类似，前者消耗后者的资源。pvc可以向pv申请指定大小的存储资源并设置访问模式。 pv 属性 storage 容量 读写属性 分别为 - ReadWriteOnce：单个节点读写 , ReadOnlyMany：多节点只读 , ReadWriteMany：多节点读写 。 123456789101112131415161718192021222324252627vi glusterfs-pv.yaml---apiVersion: v1kind: PersistentVolumemetadata: name: gluster-dev-volumespec: capacity: storage: 8Gi accessModes: - ReadWriteMany glusterfs: endpoints: &quot;glusterfs-cluster&quot; path: &quot;k8s-volume&quot; readOnly: false---# 导入 pvkubectl apply -f glusterfs-pv.yaml# 查看 pvkubectl get pv pvc 属性 访问属性 与 pv 相同 容量，向pv申请的容量 &lt;= pv 总容量 配置 pvc1234567891011121314151617181920212223---kind: PersistentVolumeClaimapiVersion: v1metadata: name: glusterfs-nginxspec: accessModes: - ReadWriteMany resources: requests: storage: 8Gi---# 导入 pvckubectl apply -f glusterfs-pvc.yaml# 查看 pvckubectl get pv# STATUS 为 Bound ， VOLUME 为 pv name 创建 nginx deployment 挂载 volume123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960vi nginx-deployment.yaml---apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-dmspec: replicas: 2 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 volumeMounts: - name: gluster-dev-volume mountPath: &quot;/usr/share/nginx/html&quot; volumes: - name: gluster-dev-volume persistentVolumeClaim: claimName: glusterfs-nginx ---# 导入 deploymentkubectl apply -f nginx-deployment.yaml # 查看 deploymentkubectl get pods |grep nginx-dmnginx-dm-2784556780-cnjdw 1/1 Running 0 8mnginx-dm-2784556780-pt8vf 1/1 Running 0 8m# 查看 挂载kubectl exec -it nginx-dm-2784556780-cnjdw -- df -h|grep k8s-volume# 创建文件 测试kubectl exec -it nginx-dm-2784556780-cnjdw -- touch /usr/share/nginx/html/index.htmlkubectl exec -it nginx-dm-2784556780-pt8vf -- ls -lt /usr/share/nginx/html/index.html# 验证 glusterfs# 因为我们使用 分布式复制卷，所以可以看到2个节点中有文件[root@gluster-1 ~] ls /opt/gfs_data/[root@gluster-2 ~] ls /opt/gfs_data/[root@gluster-3 ~] ls /opt/gfs_data/[root@gluster-4 ~] ls /opt/gfs_data/ FAQ 问题1234567891011121314151617181920212223242526272829303132333435363738394041# 在使用 pv 与 pvc 的过程中遇到问题# 第一次创建 pv 与 pvc 的时候 状态都是OK的# 当删除 pvc 以后，再创建 pvc 状态一直 pending 无论如何都不正常# 查看日志 报 no persistent volumes available for this claim and no storage class is set# 这里我们可以跳过 pv 与 pvc 只需要创建一个 ep 就可以# 挂载的时候我们直接在 yaml 目录下 写 ep 就行# 如下为 deployment 的 yaml 文件apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-dmspec: replicas: 2 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 volumeMounts: - name: gluster-efk-volume mountPath: &quot;/usr/share/nginx/html&quot; volumes: - name: gluster-efk-volume glusterfs: endpoints: glusterfs-cluster path: efk-volume readOnly: False","categories":[{"name":"docker","slug":"docker","permalink":"https://luolizhi.github.io/categories/docker/"}],"tags":[]},{"title":"nginx","slug":"nginx","date":"2018-12-01T02:50:27.000Z","updated":"2019-11-30T03:47:11.954Z","comments":true,"path":"2018/12/01/nginx/","link":"","permalink":"https://luolizhi.github.io/2018/12/01/nginx/","excerpt":"","text":"nginxnginx 安装1. yum安装123456789101112# 1.将nginx放到yum repro库中[root@localhost ~]# rpm -ivh http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm# 2.查看nginx信息[root@localhost ~]# yum info nginx# 3.使用yum安装ngnix[root@localhost ~]# yum install nginx# 4.启动nginx[root@localhost ~]# service nginx start# 5.查看nginx版本[root@localhost ~]# nginx -v# 6.访问nginx，现在你可以通过公网ip (本地可以通过 localhost /或 127.0.0.1 ) 查看nginx 服务返回的信息。[root@localhost ~]# curl -i localhost 2. 源码安装1234567891011121314# 1.下载nginx包。[root@localhost ~]# wget http://nginx.org/download/nginx-1.14.0.tar.gz#2.复制包到你的安装目录[root@localhost ~]# cp nginx-1.14.0.tar.gz /usr/local/# 3.解压[root@localhost ~]# tar -zxvf nginx-1.14.0.tar.gz[root@localhost ~]# cd nginx-1.14.0./configuremake &amp;&amp; make install# 4.启动nginx[root@localhost ~]# /usr/local/nginx/sbin/nginx# 5.查看版本s[root@localhost ~]# nginx -v#6.url访问nginx localhost或127.0.0.1 安装其他模块在上述第3步 ./configure --add-module=/path/to/module 后续继续执行即可 设置开机启动在/etc/rc.d/rc.local文件最后一行增加启动的命令/usr/local/nginx/sbin/nginxchmod +x /etc/rc.d/rc.local nginx 配置 配置文件示意图 yum安装的配置文件路径为/etc/nginx/nginx.conf 源码安装的配置文件路径为/usr/local/nginx/conf/nginx.conf 默认配置文件如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120[root@localhost ~]# cat /usr/local/nginx/conf/nginx.conf#user nobody;worker_processes 1;#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;#pid logs/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; #log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' # '$status $body_bytes_sent \"$http_referer\" ' # '\"$http_user_agent\" \"$http_x_forwarded_for\"'; #access_log logs/access.log main; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; server &#123; listen 80; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / &#123; root html; index index.html index.htm; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \\.php$ &#123; # proxy_pass http://127.0.0.1; #&#125; # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \\.php$ &#123; # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #&#125; # deny access to .htaccess files, if Apache's document root # concurs with nginx's one # #location ~ /\\.ht &#123; # deny all; #&#125; &#125;//这里可以配置其他server # another virtual host using mix of IP-, name-, and port-based configuration # #server &#123; # listen 8000; # listen somename:8080; # server_name somename alias another.alias; # location / &#123; # root html; # index index.html index.htm; # &#125; #&#125; # HTTPS server # #server &#123; # listen 443 ssl; # server_name localhost; # ssl_certificate cert.pem; # ssl_certificate_key cert.key; # ssl_session_cache shared:SSL:1m; # ssl_session_timeout 5m; # ssl_ciphers HIGH:!aNULL:!MD5; # ssl_prefer_server_ciphers on; # location / &#123; # root html; # index index.html index.htm; # &#125; #&#125; // 这里可以增加include // include /usr/local/nginx/conf.d/*.conf&#125; 修改配置可以直接在server中增加location，或者在http中增加server（参考上图和默认配置） nginx 常用操作启动 (以源码安装为例)service nginx start或者/usr/local/nginx/sbin/nginx 重启/usr/local/nginx/sbin/nginx -s reload 停止 /usr/local/nginx/sbin/nginx -s stop location 规则语法规则： location [ 空格 | = | ~ | ~ | ^~| !~ | !~ ] /uri/ { … }= 开头表示精确匹配^~ 开头表示uri以某个常规字符串开头，理解为匹配 url路径即可。nginx不对url做编码，因此请求为/static/20%/aa，可以被规则^~ /static/ /aa匹配到（注意是空格）。~ 开头表示区分大小写的正则匹配~ 开头表示不区分大小写的正则匹配!~和!~分别为区分大小写不匹配及不区分大小写不匹配 的正则/ 通用匹配，任何请求都会匹配到。多个location配置的情况下匹配顺序为：(location =) &gt; (location 完整路径) &gt; (location ^~ 路径) &gt; (location ~,~* 正则顺序) &gt; (location 部分起始路径) &gt; (/) 1.首先匹配=2.其次匹配^~3.再其次按照配置文件的顺序进行正则匹配、4.最后是交给/进行通用匹配注意：当有匹配成功时，立刻停止匹配，按照当前匹配规则处理请求 1234567891011121314151617181920212223location = / &#123; #规则A&#125;location = /login &#123; #规则B&#125;location ^~ /static/ &#123; #规则C&#125;location ~ \\.(gif|jpg|png|js|css)$ &#123; #规则D&#125;location ~* \\.png$ &#123; #规则E&#125;location !~ \\.xhtml$ &#123; #规则F&#125;location !~* \\.xhtml$ &#123; #规则G&#125;location / &#123; #规则H 那么产生的效果如下：访问根目录/， 比如http://localhost/ 将匹配规则A访问 http://localhost/login 将匹配规则B，http://localhost/register 则匹配规则H访问 http://localhost/static/a.html 将匹配规则C访问 http://localhost/a.gif, http://localhost/b.jpg 将匹配规则D和规则E，但是规则D顺序优先，规则E不起作用，而 http://localhost/static/c.png 则优先匹配到 规则C访问 http://localhost/a.PNG 则匹配规则E， 而不会匹配规则D，因为规则E不区分大小写。访问 http://localhost/a.xhtml 不会匹配规则F和规则G，http://localhost/a.XHTML 不会匹配规则G，因为不区分大小写。规则F，规则G属于排除法，符合匹配规则但是不会匹配到，所以想想看实际应用中哪里会用到。访问 http://localhost/category/id/1111 则最终匹配到规则H，因为以上规则都不匹配，这个时候应该是nginx转发请求给后端应用服务器，比如FastCGI（php），tomcat（jsp），nginx作为方向代理服务器存在。所以实际使用中，通常至少有三个匹配规则定义，如下：123456789101112131415161718192021#直接匹配网站根，通过域名访问网站首页比较频繁，使用这个会加速处理，官网如是说。#这里是直接转发给后端应用服务器了，也可以是一个静态首页# 第一个必选规则location = / &#123; proxy_pass http://tomcat:8080/index&#125; # 第二个必选规则是处理静态文件请求，这是nginx作为http服务器的强项# 有两种配置模式，目录匹配或后缀匹配,任选其一或搭配使用location ^~ /static/ &#123; root /webroot/static/;&#125;location ~* \\.(gif|jpg|jpeg|png|css|js|ico)$ &#123; root /webroot/res/;&#125; #第三个规则就是通用规则，用来转发动态请求到后端应用服务器#非静态文件请求就默认是动态请求，自己根据实际把握#毕竟目前的一些框架的流行，带.php,.jsp后缀的情况很少了location / &#123; proxy_pass http://tomcat:8080/ 显示真实ip地址在配置文件中，location，proxy_pass之前增加下面三行1234567location / &#123; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://tomcat:8080/&#125; nginx try_files 用法要使非HTML请求实际资源不存在时响应404，方法是：若请求的资源不是HTML，则放弃尝试后备文件。要使得try_files不影响index和/与autoindex，方法是：若请求的路径是目录，则放弃尝试后备文件。 123456789101112131415location / &#123; root /var/www/mysite; index index.html; autoindex on; set $fallback_file /index.html; if ($http_accept !~ text/html) &#123; set $fallback_file /null; &#125; if ($uri ~ /$) &#123; set $fallback_file $uri; &#125; try_files $uri $fallback_file;&#125; 首页在/login不跳转时增加上述配置解决问题.(微信端登录bug)","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://luolizhi.github.io/tags/linux/"},{"name":"nginx","slug":"nginx","permalink":"https://luolizhi.github.io/tags/nginx/"}]},{"title":"fastdfs 集群","slug":"fastdfs集群","date":"2018-12-01T02:50:27.000Z","updated":"2019-11-30T03:50:31.872Z","comments":true,"path":"2018/12/01/fastdfs集群/","link":"","permalink":"https://luolizhi.github.io/2018/12/01/fastdfs集群/","excerpt":"","text":"fastdfs 安装安装环境 组件 版本 操作系统 Centos7.2 Final FastDFS版本 fastdfs-5.11.tar.gz libfastcommon https://github.com/happyfish100/libfastcommon nginx nginx-1.14.0.tar.gz fastdfs-nginx-module fastdfs-nginx-module_1.21 架构app25—tracker 192.168.77.76 fastdfs，fastcommon，nginx app26—storage 192.168.77.77 fastdfs，fastcommon，nginx，fastdfs-nginx-moduleapp27—storage 192.168.77.78 fastdfs，fastcommon，nginx，fastdfs-nginx-module fdfs架构图 安装libfastcommon，fastdfs （所有节点）12345678910111213# fastcommonwget https://github.com/happyfish100/libfastcommon.git #安装libfastcommon-master(与libfastcommon不同，里面多了很多头文件)tar zxvf libfastcommon-master.tar.gzcd libfastcommon-master/./make.sh./make.sh install# fastdfs安装wget https://github.com/happyfish100/fastdfs/archive/V5.11.tar.gzcd fastdfs-5.11/./make.sh./make.sh install tracker配置 （192.168.77.76）123456mkdir -p /data/fastdfs/cd /etc/fdfs/cp tracker.conf.sample tracker.confvim tracker.conf#修改参数base_path=/data/fastdfs 创建软链接 123ln -s /usr/bin/fdfs_trackerd /usr/local/bin/ln -s /usr/bin/stop.sh /usr/local/bin/ln -s /usr/bin/restart.sh /usr/local/bin/ 启动服务service fdfs_trackerd start 或者 /usr/bin/fdfs_trackerd /etc/fdfs/tracker.conf restart ` 查看监听netstat -nltp|grep fdfs storage配置（192.168.77.77，192.168.77.78）1234567891011121314mkdir -p /data/fastdfs/storagecd /etc/fdfs/cp storage.conf.sample storage.confvim storage.conf#修改下面参数# the base path to store data and log filesbase_path=/data/fastdfs/storage# store_path#, based 0, if store_path0 not exists, it&apos;s value is base_path# the paths must be existstore_path0=/data/fastdfs/storage#store_path1=/home/yuqing/fastdfs2# tracker_server can ocur more than once, and tracker_server format is# &quot;host:port&quot;, host can be hostname or ip addresstracker_server=192.168.77.76:22122 创建软链接ln -s /usr/bin/fdfs_storaged /usr/local/bin/ 启动服务service fdfs_storaged start 或者/usr/bin/fdfs_storaged /etc/fdfs/storage.conf restart 查看监听/usr/bin/fdfs_monitor /etc/fdfs/storage.conf 设置开机启动123vim /etc/rc.d/rc.local #最后添加 /etc/init.d/fdfs_storaged start 省略软连接设置自启动echo &#39;/usr/bin/fdfs_storaged /etc/fdfs/storage.conf restart&#39; &gt;&gt; /etc/rc.d/rc.local 上传下载通常，对于图片和文件的访问，是不太可能走TCP，而是通过简单的HTTP访问，这时需要通过一些Web服务器(如nginx，apache)来代理，fastdfs也有了nginx的支持，下面则将通过安装nginx来完成文件访问，之前的开发环境将变为： 此时已经可以上传，下载还需要安装nginx和fastdfs-nginx-module，安装完成后一起测试 完整的文件服务器架构图： nginx安装 tracker上只安装nginx1234567# 源码安装wget http://nginx.org/download/nginx-1.14.0.tar.gzsudo apt-get install -y libpcre3 libpcre3-dev zlib1g-dev openssl libssl-dev //安装依赖库yum -y install pcre-devel opensslopenssl-devel zlib-devel //centos7 安装依赖库cd nginx-1.14.0/./configuremake &amp;&amp; make install 主要配置文件：/usr/local/nginx/conf/nginx.conf /etc/init.d/下创建了启动脚本nginx（需手动配置）程序文件放在/usr/local/nginx/sbin/nginx日志放在了/var/log/nginx中虚拟主机的目录设置在/var/www/下 默认启动/usr/local/nginx/sbin/nginx nginx配置在默认配置文件（/usr/local/nginx/conf/nginx.conf）中80行增加include /usr/local/nginx/conf.d/*.conf ; 1234567891011121314151617181920mkdir -p /usr/local/nginx/conf.dcd /usr/local/nginx/conf.dvim fastdfs.conf# group1的Storage集群upstream group1_cluster &#123; server 10.0.0.132; server 10.0.0.134;&#125;server &#123; listen 80; server_name 10.0.0.133; location /group1 &#123; proxy_pass http://group1_cluster; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; &#125;&#125; storage上安装nginx和fastdfs-nginx-module （192.168.77.77，192.168.77.78）将fastdfs-nginx-module目录下面的mod_fastdfs.conf拷贝到/etc/fdfs/将fastdfs-5.11目录下面的http.conf，mime.types拷贝到/etc/fdfs/12345678910# 配置fastdfs-nginx-module所需的配置文件mod_fastdfs.conf，http.conf，mime.types# vim /etc/fdfs/mod_fastdfs.confbase_path=/data/fastdfs/storagetracker_server=10.0.0.133:22122group_name=group1url_have_group_name = truestore_path_count=1store_path0=/data/fastdfs/storage 源码安装nginx123#下载解压安装依赖库同上面nginx安装./configure --add-module=../fastdfs-nginx-module-master/src/make &amp;&amp; make install 可能的报错12345/usr/include/fastdfs/fdfs_define.h:15:27: fatal error: common_define.h: No such file or directory #include &quot;common_define.h&quot;#解决方案cp /usr/include/fastcommon/* /usr/include/ nginx最简单配置1234567891011121314151617181920212223242526# 最简配置/usr/local/nginx/conf/nginx.conf# user nginx;worker_processes 1;events &#123; worker_connections 1024;&#125; http &#123; server &#123; listen 80; server_name localhost; # group1为该Storage所在的group location ~ /group1/M00 &#123; # 该Storage的data目录 root /mnt/fastdfs/data; # 由于fastdfs保存的文件名已经编码，源文件名将丢失，应用可通过在请求url后加oname参数指定源文件名 if ($arg_oname != &apos;&apos;)&#123; add_header Content-Disposition &quot;attachment;filename=$arg_oname&quot;; &#125; # 调用nginx-fastdfs-module模块 ngx_fastdfs_module; &#125; &#125;&#125; 启动nginx,不报错即成功/usr/local/nginx/sbin/nginx 开机自启动echo &#39;/usr/local/nginx/sbin/nginx &#39; &gt;&gt; /etc/rc.d/rc.local 在tracker上操作集群，上传下载修改/etc/fdfs/client.conf12base_path=/data/fastdfstracker_server=10.0.0.133:22122 查看集群1/usr/bin/fdfs_monitor /etc/fdfs/client.conf 文件上传1234567891011121314151617181920212223242526272829303132333435/usr/bin/fdfs_test /etc/fdfs/client.conf upload /root/test/hello.txt#结果This is FastDFS client test program v5.11Copyright (C) 2008, Happy Fish / YuQingFastDFS may be copied only under the terms of the GNU GeneralPublic License V3, which may be found in the FastDFS source kit.Please visit the FastDFS Home Page http://www.csource.org/ for more detail.[2018-07-31 07:38:11] DEBUG - base_path=/data/fastdfs, connect_timeout=30, network_timeout=60, tracker_server_count=1, anti_steal_token=0, anti_steal_secret_key length=0, use_connection_pool=0, g_connection_pool_max_idle_time=3600s, use_storage_id=0, storage server id count: 0tracker_query_storage_store_list_without_group: server 1. group_name=, ip_addr=10.0.0.134, port=23000 server 2. group_name=, ip_addr=10.0.1.92, port=23000group_name=group1, ip_addr=10.0.0.134, port=23000storage_upload_by_filenamegroup_name=group1, remote_filename=M00/00/00/CgAAhltgEeOAGIdUAAAADBSRxKc586.txtsource ip address: 10.0.0.134file timestamp=2018-07-31 07:38:11file size=12file crc32=345097383example file url: http://10.0.0.134/group1/M00/00/00/CgAAhltgEeOAGIdUAAAADBSRxKc586.txtstorage_upload_slave_by_filenamegroup_name=group1, remote_filename=M00/00/00/CgAAhltgEeOAGIdUAAAADBSRxKc586_big.txtsource ip address: 10.0.0.134file timestamp=2018-07-31 07:38:11file size=12file crc32=345097383example file url: http://10.0.0.134/group1/M00/00/00/CgAAhltgEeOAGIdUAAAADBSRxKc586_big.txt 文件下载12345/usr/bin/fdfs_test /etc/fdfs/client.conf download group1 M00/00/00/CgAAhltgEeOAGIdUAAAADBSRxKc586.txt结果storage=10.0.0.134:23000download file success, file size=12, file save to CgAAhltgEeOAGIdUAAAADBSRxKc586.txt 配置详解FastDFS的配置、部署与API使用解读（6）FastDFS配置详解之Storage配置 - CSDN博客","categories":[],"tags":[{"name":"fastdfs","slug":"fastdfs","permalink":"https://luolizhi.github.io/tags/fastdfs/"}]},{"title":"centos搭建ftp服务器","slug":"centos7安装ftp服务器","date":"2018-12-01T02:50:27.000Z","updated":"2019-11-30T03:53:55.469Z","comments":true,"path":"2018/12/01/centos7安装ftp服务器/","link":"","permalink":"https://luolizhi.github.io/2018/12/01/centos7安装ftp服务器/","excerpt":"","text":"centos搭建ftp服务器系统环境Centos7.2 安装步骤通过yum来安装vsftpdsudo yum -y install vsftpd 设置为开机启动sudo chkconfig vsftpd on 修改配置vim /etc/vsftpd/vsftpd.conf 修改如下123456local_enable=YESwrite_enable=YESlocal_umask=022chroot_local_user=YES #这行可能需自己写pam_service_name=vsftpduserlist_enable=YES 注：chroot_local_user #是否将所有用户限制在主目录,YES为启用 NO禁用.(该项默认值是NO,即在安装vsftpd后不做配置的话，ftp用户是可以向上切换到要目录之外的) 配置保存后，重启vsftpd服务sudo service vsftpd restart 添加用户添加vsftpd账号,并制定ftpsudo useradd d /home/ftpdir -s /sbin/nologin vsftpd 为账号设置密码，按提示操作sudo passwd vsftpd为用户的目录修改权限，实现上传和下载文件sudo chmod o+w /home/ftpdir 配置Centos防火墙添加ip_conntrack_ftp模块sudo vi /etc/sysconfig/iptables-config 添加下面一行IPTABLES_MODULES=&quot;ip_conntrack_ftp&quot; 打开ftp端口21sudo vi /etc/sysconfig/iptables添加下面一行 -A INPUT -m state --state NEW -m tcp -p tcp --dport 21 -j ACCEPT 重启iptables使新的规则生效sudo service iptables restart 测试可用WinSCP进行测试","categories":[],"tags":[{"name":"ftp","slug":"ftp","permalink":"https://luolizhi.github.io/tags/ftp/"}]},{"title":"ansible 基本语法","slug":"ansible基本语法","date":"2018-12-01T02:50:27.000Z","updated":"2019-11-30T03:55:34.330Z","comments":true,"path":"2018/12/01/ansible基本语法/","link":"","permalink":"https://luolizhi.github.io/2018/12/01/ansible基本语法/","excerpt":"","text":"ansible 基本语法Ansible中文权威指南Ansible教程Ansible基础学习 command模块：是Ansible默认执行模块，在远程主机执行 shell 指令用法下面两种方法等价，默认执行commandansible all -a &quot;pwd&quot;ansible all -m command -a &quot;pwd&quot; 12345678# 关闭系统-command: /sbin/shutdown -t now# 当指定的文件存在时，跳过不执行该命令-command: /usr/bin/make_database.sh arg1 arg2 creates=/path/to/database# 当指定不文件存在时，则该命令将会执行-command: /usr/bin/make_database.sh arg1 arg2 creates=/path/to/database 注：这个模块不支持管道和重定向，要支持管道可以用-m shell 或者-m raw，shell 和 raw 语法一样，它们支持管道和重定向，如：123- shell: ifconfig|grep inet|grep -Ev '127.'|cut -d':' -f2|cut -d' ' -f1- raw: ifconfig|grep inet|grep -Ev '127.'|cut -d':' -f2|cut -d' ' -f1 &gt; /tmp/ip.log file模块：设置文件的属性 用法ansible all -m stat -a &quot;path=/root/ansible&quot; (1) 创建目录： -a &quot;path= state=directory&quot; (2) 创建链接文件： -a &quot;path= src= state=link&quot; (3) 删除文件： -a &quot;path= state=absent&quot; 1234567891011121314151617181920# 创建文件夹,recurse=yes 递归，相当于 mkdir -p- file: path=/root/soft/ state=directory mode=0755 recurse=yes# 创建文件- file: path=/etc/foo.conf state=touch mode=\"u=rw,g-wx,o-rwx\"# 删除文件- file: path=/root/soft state=absent # 修改文件目录属性- file: path=/root/soft mode=750 owner=ops group=ops# 创建文件软件链接- file: src=/file/link dest=/path/symlink owner=foo group=foo state=link# 批量创建文件软件链接- file: src=/tmp/&#123;&#123; item.src &#125;&#125; dest=&#123;&#123; item.dest &#125;&#125; state=link with_items: - &#123; src: 'x', dest: 'y' &#125; - &#123; src: 'z', dest: 'k' &#125; 12345678910111213141516171819202122232425262728293031323334353637[root@localhost ansible]# ansible all -m file -a \"path=/root/ansible state=directory\"192.168.101.229 | SUCCESS =&gt; &#123; \"changed\": true, \"gid\": 0, \"group\": \"root\", \"mode\": \"0755\", \"owner\": \"root\", \"path\": \"/root/ansible\", \"secontext\": \"unconfined_u:object_r:admin_home_t:s0\", \"size\": 6, \"state\": \"directory\", \"uid\": 0&#125;192.168.101.104 | SUCCESS =&gt; &#123; \"changed\": true, \"gid\": 0, \"group\": \"root\", \"mode\": \"0755\", \"owner\": \"root\", \"path\": \"/root/ansible\", \"secontext\": \"unconfined_u:object_r:admin_home_t:s0\", \"size\": 6, \"state\": \"directory\", \"uid\": 0&#125;192.168.100.197 | SUCCESS =&gt; &#123; \"changed\": false, \"gid\": 0, \"group\": \"root\", \"mode\": \"0755\", \"owner\": \"root\", \"path\": \"/root/ansible\", \"secontext\": \"unconfined_u:object_r:admin_home_t:s0\", \"size\": 21, \"state\": \"directory\", \"uid\": 0&#125; copy模块：复制文件到远程主机 用法`ansible all -m copy -a “src=/root/ansible/testfile dest=/root/ansible/testfile mode=600”12345678910111213141516171819# 复制 redis 服务脚本到远程主机机器，force 为强制性复制- copy: src=/data/ansible/file/redis/redis-server dest=/etc/init.d/ mode=750 owner=root group=root force=yes# 复制目录，directory_mode 目录模式- copy: src=/root/nginx dest=/root/nginx mode=750 owner=ops group=ops directory_mode=yes# 复制前对文件进行备份，backup=yes- copy: src=/srv/myfiles/foo.conf dest=/etc/foo.conf owner=foo group=foo mode=\"u+rw,g-wx,o-rwx\" backup=yes# 多个文件的复制，采用循环 with_items- copy: src=&#123;&#123; item &#125;&#125; dest=/root/ with_items: - /data/a.sh - /logs/b.py# 文件通配符循环 with_fileglob- copy: src=&#123;&#123; item &#125;&#125; dest=/etc/fooapp/ owner=root mode=600 with_fileglob: - /playbooks/files/fooapp/* 123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@localhost ansible]# ansible all -m copy -a \"src=/root/ansible/testfile dest=/root/ansible/testfile mode=600\"192.168.101.229 | SUCCESS =&gt; &#123; \"changed\": true, \"checksum\": \"795f9549ca9d7715279a63ff49b5b801da3ec0ad\", \"dest\": \"/root/ansible/testfile\", \"gid\": 0, \"group\": \"root\", \"md5sum\": \"82f25f5116f96ba86889e004f41ecb86\", \"mode\": \"0600\", \"owner\": \"root\", \"secontext\": \"system_u:object_r:admin_home_t:s0\", \"size\": 22, \"src\": \"/root/.ansible/tmp/ansible-tmp-1533262319.99-21402740317520/source\", \"state\": \"file\", \"uid\": 0&#125;192.168.101.104 | SUCCESS =&gt; &#123; \"changed\": true, \"checksum\": \"795f9549ca9d7715279a63ff49b5b801da3ec0ad\", \"dest\": \"/root/ansible/testfile\", \"gid\": 0, \"group\": \"root\", \"md5sum\": \"82f25f5116f96ba86889e004f41ecb86\", \"mode\": \"0600\", \"owner\": \"root\", \"secontext\": \"system_u:object_r:admin_home_t:s0\", \"size\": 22, \"src\": \"/root/.ansible/tmp/ansible-tmp-1533262319.98-120771157809251/source\", \"state\": \"file\", \"uid\": 0&#125;192.168.100.197 | SUCCESS =&gt; &#123; \"changed\": false, \"checksum\": \"795f9549ca9d7715279a63ff49b5b801da3ec0ad\", \"dest\": \"/root/ansible/testfile\", \"gid\": 0, \"group\": \"root\", \"mode\": \"0600\", \"owner\": \"root\", \"path\": \"/root/ansible/testfile\", \"secontext\": \"unconfined_u:object_r:admin_home_t:s0\", \"size\": 22, \"state\": \"file\", \"uid\": 0&#125; libel stat模块用法ansible all -m stat -a &quot;path=/root/ansible&quot; 判断文件是否存在12345678910111213141516171819# 判断一个路径是存在，且是一个目录- stat: path=/path/to/something register: reg- debug: msg=\"Path exists and is a directory\" when: reg.stat.isdir is defined and p.stat.isdir# 判断文件属主是否发生改变- stat: path=/etc/foo.conf register: reg- fail: msg=\"Oh,file ownership has changed\" when: re.stat.pw_name != \"root\"# 判断文件是否存- stat: path=/path/to/something register: reg- debug: msg=\"file doesn't exist\" when: not reg.stat.exists- debug: msg=\"file is exist\" when: reg.stat.exists hostname模块：管理主机名 用法 name=appserver 是group name，在/etc/ansible/hosts中设置12345678910111213141516171819[root@localhost ansible]# ansible all -a \"hostname\"192.168.101.104 | SUCCESS | rc=0 &gt;&gt;localhost.localdomain192.168.100.197 | SUCCESS | rc=0 &gt;&gt;localhost.localdomain192.168.101.229 | SUCCESS | rc=0 &gt;&gt;localhost.localdomain[root@localhost ansible]# ansible appserver -a \"hostname\" 192.168.101.229 | SUCCESS | rc=0 &gt;&gt;localhost.localdomain192.168.101.104 | SUCCESS | rc=0 &gt;&gt;localhost.localdomain192.168.100.197 | SUCCESS | rc=0 &gt;&gt;localhost.localdomain yum模块：使用yum命令完成程序包管理 用法ansible all -m yum -a &quot;name=lrzsz&quot; 123456789101112131415161718192021222324- name: 安装最新版本的 Apache yum: name=httpd state=latest- name: 删除 Apache yum: name=httpd state=absent- name: 从指定的 yum 源安装最新版本的 Apache yum: name=httpd enablerepo=epel state=present- name: 安装指定 url 的 RPM 包yum: name=http://mirrors.sohu.com/fedora-epel/6/x86_64/epel-release-6-8.noarch.rpm state=present- name: 安装指定版本号的 Apache yum: name=httpd-2.2.29-1.4.amzn1 state=present- name: 更新系统所有的包 yum: name=* state=latest- name: 从本地路径上安装指定的 nginx yum: name=/usr/local/src/nginx-release-centos-6-0.el6.ngx.noarch.rpm state=present- name: 安装系统开发工具包 yum: name=\"@Development tools\" state=present 12ansible all -m yum -a \"name=samba\" //安装ansible all -m yum -a \"name=samba state=removed\" //卸载 service模块：主要用于系统服务管理,包括启动、关闭、重启、以及设置开机启动等 用法ansible all -m service -a &quot;name=vdftpd state=started enabled=yes&quot;1234567891011# 启动服务,并设置开机启动- service: name=vsftpd state=started enabled=yes# 关闭服务- service: name=vsftpd state=stopped# 重启服务- service: name=vsftpd state=restarted# 重载服务- service: name=vsftpd state=reloaded group模块：增加或删除组用法ansible all -m group -a &quot;name=ftp gid=1024 state=present&quot; 12345# 创建用户组group: name=lipeibin gid=1024 state=present# 删除用户组group: name=lipeibin state=absent user模块：用户管理用法ansible all -m user -a &quot;name=ftp groups=ftp&quot; 1234567891011# 创建用户 lipeibin- user: name=lipeibin groups=lipeibin shell=/sbin/nologin state=present# 删除用户 lipeibin,并删除用户主目录/home/lipeibin- user: name=lipeibin state=absent remove=yes# 创建用户 lipeibin,并且设置密码，归属 ops 组- user: name=lipeibin password=”19890506”| password_hash('sha512') group=ops# 设置用户有效时间- user: name=lipeibin shell=/bin/bash groups=ops expires=1422403387 123ansible all -m file -a &quot;path=/root/ansible state=directory&quot; //在所有节点上创建文件夹 -file模块ansible all -m copy -a &quot;src=/root/ansible/testfile dest=/root/ansible/testfile mode=600&quot; //复制文件到所有节点 copy模块ansible all -m shell -a &quot;netstat -nltp | grep 80&quot; //检测端口 replace模块主要用于搜索匹配替换，类似于 linux 命令 sed用法ansible all -m replace ???123456# 把 old.host.name 替换成 new.host.name- replace: dest=/etc/hosts regexp='(\\s+)old\\.host\\.name(\\s+.*)?$' replace='\\1new.host.name\\2' owner=jdoe group=jdoe mode=644 backup=yes# 把 Listen 80 和 NameVirtualHost 80 分别替换成 Listen 127.0.0.1:8080 和 NameVirtualHost 127.0.0.1:8080 - replace: dest=/etc/apache/ports regexp='^(NameVirtualHost|Listen)\\s+80\\s*$' replace='\\1 127.0.0.1:8080'' PlayBook核心元素：Tasks：任务，由模块定义的操作的列表； Variables：变量 Templates：模板，即使用了模板语法的文本文件； Handlers：由特定条件触发的Tasks； Roles：角色； playbook的基础组件： Hosts：运行指定任务的目标主机； remote_user：在远程主机以哪个用户身份执行； sudo_user：非管理员需要拥有sudo权限； tasks：任务列表 模块，模块参数： 格式： (1) action: module arguments (2) module: arguments 运行playbook，使用ansible-playbook命令(1) 检测语法 ansible-playbook --syntax-check /path/to/playbook.yaml (2) 测试运行 -C 是测试 ansible-playbook -C /path/to/playbook.yaml --list-hosts --list-tasks --list-tags (3) 运行 ansible-playbook /path/to/playbook.yaml -t TAGS, --tags=TAGS --skip-tags=SKIP_TAGS --start-at-task=START_AT 1ansible-playbook -C group.yml tags：给指定的任务定义一个调用标识； 使用格式： - name: NAME module: arguments tags: TAG_ID Variables：变量 类型： 内建： (1) facts 自定义： (1) 命令行传递； -e VAR=VALUE (2) 在hosts Inventory中为每个主机定义专用变量值； (a) 向不同的主机传递不同的变量 ； IP/HOSTNAME variable_name=value (b) 向组内的所有主机传递相同的变量 ； [groupname:vars] variable_name=value (3) 在playbook中定义 vars: - var_name: value - var_name: value (4) Inventory还可以使用参数： 用于定义ansible远程连接目标主机时使用的属性，而非传递给playbook的变量； ansible_ssh_host ansible_ssh_port ansible_ssh_user ansible_ssh_pass ansible_sudo_pass ... (5) 在角色调用时传递 roles: - { role: ROLE_NAME, var: value, ...} 变量调用： ` var_name `","categories":[],"tags":[{"name":"ansible","slug":"ansible","permalink":"https://luolizhi.github.io/tags/ansible/"},{"name":"linux","slug":"linux","permalink":"https://luolizhi.github.io/tags/linux/"}]},{"title":"centos7安装apache文件下载服务器","slug":"centos7安装apache文件下载服务器","date":"2018-12-01T02:50:27.000Z","updated":"2019-11-30T03:54:49.615Z","comments":true,"path":"2018/12/01/centos7安装apache文件下载服务器/","link":"","permalink":"https://luolizhi.github.io/2018/12/01/centos7安装apache文件下载服务器/","excerpt":"","text":"centos7安装apache文件下载服务器part.1安装httpdyum install -y httpd part.2 设置开机启动项systemctl enable httpd` part.3 修改配置文件配置文件地址为：/etc/httpd/conf/httpd.conf修改配置文件Listen 80vim /etc/httpd/conf/httpd.conf例：Listen 8051 //80为默认访问端口,若该为其它端口，首先要保证端口没有被占用，访问时也需要加端口号 part.4指定提供下载的目录地址因apache服务显示的地址默认为 /var/www/html我们进入此地址 cd /var/www/html建立文件目录软链接输入ln -s 文件目录地址 下载地址例： ln -s /home/downloads downloads即在/var/www/html目录建立新的文件夹downloads,并且链接到/home/downloads目录 part.5 启动服务查看效果systemctl httpd start访问此文件目录进行下载即在浏览器输入http://[ip]/downloads即可看到/home/downloads目录下的所有文件和文件夹 如果显示访问超时，需要把防火墙关闭。 systemctl stop firewalld.service再次访问即可。","categories":[],"tags":[{"name":"apache","slug":"apache","permalink":"https://luolizhi.github.io/tags/apache/"}]},{"title":"centos7 安装ansible","slug":"centos7安装ansible","date":"2018-12-01T02:50:27.000Z","updated":"2019-11-30T03:51:31.635Z","comments":true,"path":"2018/12/01/centos7安装ansible/","link":"","permalink":"https://luolizhi.github.io/2018/12/01/centos7安装ansible/","excerpt":"","text":"centos7 安装ansible1. 每个节点安装 依赖工具123456# 文档中脚本默认均以root用户执行# 安装 epel 源并更新yum install epel-release -yyum update# 安装pythonyum install python -y 2. 在deploy 节点安装及准备ansible1yum install -y ansible 3.在deploy节点配置免密码登陆12ssh-keygen -t rsa -b 2048 回车 回车 回车ssh-copy-id $IPs #$IPs为所有节点地址包括自身，按照提示输入yes 和root密码 批量添加密码的脚本1234567#!/bin/bashfor i in $(seq 111 116);doip=&quot;10.0.1.&quot;$i;echo $ipsshpass -p &apos;123456&apos; ssh-copy-id -o StrictHostKeyChecking=no $ipdone 4. 验证ansible安装，正常能看到每个节点返回 SUCCESS1ansible all -m ping","categories":[],"tags":[{"name":"ansible","slug":"ansible","permalink":"https://luolizhi.github.io/tags/ansible/"}]},{"title":"kubernetes源码结构","slug":"kubernetes源码结构","date":"2018-04-11T06:24:00.000Z","updated":"2019-11-29T11:45:41.530Z","comments":true,"path":"2018/04/11/kubernetes源码结构/","link":"","permalink":"https://luolizhi.github.io/2018/04/11/kubernetes源码结构/","excerpt":"","text":"kubernetes源码地址：https://github.com/kubernetes/kubernetes 整体结构kubernetes源码主要分为包括pkg、cmd、plugin、test四个目录。其中各个包的主要功能如下： 1.pkg是kubernetes的主体代码，里面实现了kubernetes的主体逻辑。 2.cmd是kubernetes的所有后台进程的代码，主要是各个子模块的启动代码，具体的实现逻辑在pkg下。 3.plugin主要是kube-scheduler和一些插件 主要包功能介绍以下简要介绍一下各个子包的功能 pkg 包名 用途 api kubernetes api主要包括最新版本的Rest API接口的类，并提供数据格式验证转换工具类，对应版本号文件夹下的文件描述了特定的版本如何序列化存储和网络 client Kubernetes 中公用的客户端部分，实现对对象的具体操作增删该查操作 cloudprovider kubernetes 提供对aws、azure、gce、cloudstack、mesos等云供应商提供了接口支持，目前包括负载均衡、实例、zone信息、路由信息等 controller kubernetes controller主要包括各个controller的实现逻辑，为各类资源如replication、endpoint、node等的增删改等逻辑提供派发和执行 credentialprovider kubernetes credentialprovider 为docker 镜像仓库贡献者提供权限认证 generated kubernetes generated包是所有生成的文件的目标文件，一般这里面的文件日常是不进行改动的 kubectl kuernetes kubectl模块是kubernetes的命令行工具，提供apiserver的各个接口的命令行操作，包括各类资源的增删改查、扩容等一系列命令工具 kubelet kuernetes kubelet模块是kubernetes的核心模块，该模块负责node层的pod管理，完成pod及容器的创建，执行pod的删除同步等操作等等 master kubernetes master负责集群中master节点的运行管理、api安装、各个组件的运行端口分配、NodeRegistry、PodRegistry等的创建工作 runtime kubernetes runtime实现不同版本api之间的适配，实现不同api版本之间数据结构的转换 cmd包括kubernetes所以后台进程的代码包括apiserver、controller manager、proxy、kubelet等进程 plugin主要包括调度模块的代码实现，用于执行具体的Scheduler的调度工作，认证等。","categories":[],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://luolizhi.github.io/tags/kubernetes/"},{"name":"源码","slug":"源码","permalink":"https://luolizhi.github.io/tags/源码/"}]},{"title":"Kubernetes中pod时区，与node主机同步","slug":"Kubernetes中pod时区，与node主机同步","date":"2018-04-03T02:52:04.000Z","updated":"2019-11-29T11:45:41.524Z","comments":true,"path":"2018/04/03/Kubernetes中pod时区，与node主机同步/","link":"","permalink":"https://luolizhi.github.io/2018/04/03/Kubernetes中pod时区，与node主机同步/","excerpt":"","text":"k8s集群中的pod时间与主机不同步，解决这个问题基本上可以有两种思路： 直接修改镜像的时间设置，好处是应用部署时无需做特殊设置，但是需要手动构建Docker镜像。 部署应用时，单独读取主机的“/etc/localtime”文件，即创建pod时同步时区，无需修改镜像，但是每个应用都要单独设置。 这里为了快速、简单的解决这个问题，先使用第二种方案，yaml文件中设置时区同步，只需要映射主机的“/etc/localtime”文件。 这里给出一个demo123456789101112131415161718192021222324252627282930313233343536373839404142434445# test-jar-service.yamlapiVersion: v1kind: Servicemetadata: name: test-jar labels: app: test-jarspec: # if your cluster supports it, uncomment the following to automatically create # an external load-balanced IP for the frontend service. type: NodePort ports: - port: 8080# targetPort: 8080 selector: app: test-jar---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: test-jarspec: replicas: 3 template: metadata: labels: app: test-jar spec: containers: - name: test-jar image: lukey123/kubernetes:test-jar# resources:# requests:# cpu: 100m# memory: 100Mi# env:# - name: GET_HOSTS_FROM# value: dns # If your cluster config does not include a dns service, then to # instead access environment variables to find service host # info, comment out the 'value: dns' line above, and uncomment the # line below: # value: env ports: - containerPort: 8080 修改后的yaml文件为 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051apiVersion: v1kind: Servicemetadata: name: test-jar labels: app: test-jarspec: # if your cluster supports it, uncomment the following to automatically create # an external load-balanced IP for the frontend service. type: NodePort ports: - port: 8080# targetPort: 8080 selector: app: test-jar---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: test-jarspec: replicas: 3 template: metadata: labels: app: test-jar spec: containers: - name: test-jar image: lukey123/kubernetes:test-jar volumeMounts: #这里是增加的部分 - name: host-time mountPath: /etc/localtime# resources:# requests:# cpu: 100m# memory: 100Mi# env:# - name: GET_HOSTS_FROM# value: dns # If your cluster config does not include a dns service, then to # instead access environment variables to find service host # info, comment out the 'value: dns' line above, and uncomment the # line below: # value: env ports: - containerPort: 8080 volumes: #这里是增加的部分 - name: host-time hostPath: path: /etc/localtime 接着进入pod执行就可以看到时间已经同步了123$ kubectl exec -it test-jar-57d5474cbc-dv7xr /bin/sh# dateTue Apr 3 10:40:15 CST 2018","categories":[],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://luolizhi.github.io/tags/kubernetes/"},{"name":"pod","slug":"pod","permalink":"https://luolizhi.github.io/tags/pod/"}]},{"title":"Jenkins","slug":"jenkins X","date":"2018-03-27T09:19:12.000Z","updated":"2019-11-29T11:45:41.528Z","comments":true,"path":"2018/03/27/jenkins X/","link":"","permalink":"https://luolizhi.github.io/2018/03/27/jenkins X/","excerpt":"","text":"Jenkins X 是一个高度集成化的CI/CD平台，基于Jenkins和Kubernetes实现，旨在解决微服务体系架构下的云原生应用的持续交付的问题，简化整个云原生应用的开发、运行和部署过程。 Jenkins X 部分新特性1.自动化一切：自动化CI/CD流水线选择项目类型自动生成Jenkinsfile定义流水线 自动生成Dockerfile并打包容器镜像 自动创建Helm Chart并运行在Kubernetes集群 自动关联代码库和流水线，作为代码变更自动触发（基于Webhook实现） 自动版本号自动归档 2. Review代码一键部署应用：基于GitOps的环境部署所有的环境，应用列表，版本，配置信息统一放在代码库中进行版本控制 通过Pull Request实现研发和运维的协同，完成应用部署升级（Promotion） 可自动部署和手动部署，在必要的时候增加手工Review 当然这些都封装在jx命令中实现 3. 自动生成预览环境和信息同步反馈预览环境用于代码Review环节中临时创建 同Pull Request工作流程集成并实现信息同步和有效通知 验证完毕后自动清理 提交和应用状态自动同步到Github注释 自动生成release notes信息供验证 Jenkins X安装—以linux为例 在本地安装jx命令行工具(都要安装) 12$curl -L https://github.com/jenkins-x/jx/releases/download/v1.2.6/jx-linux-amd64.tar.gz | tar xzv $sudo mv jx /usr/local/bin 使用jx创建一个k8s集群，并自动安装Jenkins X http://jenkins-x.io/getting-started/create-cluster/ 在已经存在的k8s集群上安装Jenkns x http://jenkins-x.io/getting-started/install-on-cluster/jx install","categories":[],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://luolizhi.github.io/tags/k8s/"},{"name":"jenkins","slug":"jenkins","permalink":"https://luolizhi.github.io/tags/jenkins/"}]},{"title":"Kubernetes ZooKeeper K8SZK","slug":"Kubernetes-ZooKeeper-K8SZK","date":"2018-03-26T01:05:24.000Z","updated":"2019-11-29T11:45:41.523Z","comments":true,"path":"2018/03/26/Kubernetes-ZooKeeper-K8SZK/","link":"","permalink":"https://luolizhi.github.io/2018/03/26/Kubernetes-ZooKeeper-K8SZK/","excerpt":"","text":"By default, this user is zookeeper.The ZooKeeper package is installed into the /opt/zookeeper directory,all configuration is sym linked into the /usr/etc/zookeeper/,and all executables are sym linked into /usr/bin.The ZooKeeper data directories are contained in /var/lib/zookeeper.This is identical to the RPM distribution that users should be familiar with. Headless Service123456789101112131415apiVersion: v1kind: Servicemetadata: name: zk-svc labels: app: zk-svcspec: ports: - port: 2888 name: server - port: 3888 name: leader-election clusterIP: None selector: app: zk-svc Stateful Set1234567apiVersion: apps/v1beta1kind: StatefulSetmetadata: name: zkspec: serviceName: zk-svc replicas: 3 Container Configuration1234567891011121314151617181920containers: - name: k8szk imagePullPolicy: Always image: gcr.io/google_samples/k8szk:v3 ports: - containerPort: 2181 name: client - containerPort: 2888 name: server - containerPort: 3888 name: leader-election env: - name : ZK_ENSEMBLE value: &quot;zk-0;zk-1;zk-2&quot; - name: ZK_CLIENT_PORT value: &quot;2181&quot; - name: ZK_SERVER_PORT value: &quot;2888&quot; - name: ZK_ELECTION_PORT value: &quot;3888&quot;","categories":[],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://luolizhi.github.io/tags/kubernetes/"},{"name":"zookeeper","slug":"zookeeper","permalink":"https://luolizhi.github.io/tags/zookeeper/"}]},{"title":"fluentd","slug":"fluentd","date":"2018-03-23T05:36:46.000Z","updated":"2019-11-29T11:45:41.527Z","comments":true,"path":"2018/03/23/fluentd/","link":"","permalink":"https://luolizhi.github.io/2018/03/23/fluentd/","excerpt":"","text":"kubernetes 安装fluentd收集日志原理通过在每台node上部署一个以DaemonSet方式运行的fluentd来收集每台node上的日志。Fluentd将docker日志目录/var/lib/docker/containers和/var/log目录挂载到Pod中，然后Pod会在node节点的/var/log/pods目录中创建新的目录，可以区别不同的容器日志输出，该目录下有一个日志文件链接到/var/lib/docker/contianers目录下的容器日志输出。 k8s集群部署步骤–kafka 使用~/k8s/fluentd/fluentd-kubernetes-daemonset/docker-image/v0.12/alpine-kafka文件夹下面的dockerfile，构建镜像。 使用~/k8s/fluentd/fluentd-kubernetes-daemonset/my-yaml的yaml文件部署1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: fluentd namespace: kube-system labels: k8s-app: fluentd-logging version: v1 kubernetes.io/cluster-service: &quot;true&quot;spec: template: metadata: labels: k8s-app: fluentd-logging version: v1 kubernetes.io/cluster-service: &quot;true&quot; spec: tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: fluentd image: k8s/fluent/fluentd-kubernetes-daemonset:kafka env: - name: FLUENT_KAFKA_BROKERS value: &quot;10.0.0.24:9092&quot; - name: FLUENT_KAFKA_SCHEME value: &quot;http&quot; resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true nodeSelector: kubernetes.io/hostname: &quot;10.0.1.65&quot; terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers 修改环境变量FLUENT_KAFKA_BROKERS的值，镜像中会调用这个值。镜像中flentd.congf文件内容为：1234567891011121314151617181920212223242526@include kubernetes.conf&lt;match **&gt; @type kafka_buffered # list of seed brokers brokers &quot;#&#123;ENV[&apos;FLUENT_KAFKA_BROKERS&apos;]&#125;&quot; # buffer settings buffer_type file buffer_path /var/log/td-agent/buffer/td flush_interval 3s # topic settings default_topic test # data type settings output_data_type json compression_codec gzip # producer settings max_send_retries 1 required_acks -1&lt;/match&gt; default_topic的值也是可以作用环境变量。具体可以参见下面设置环境变量，然后通过yaml文件注入12345678910111213141516171819202122232425@include kubernetes.conf&lt;match **&gt; type kafka_buffered brokers &quot;#&#123;ENV[&apos;FLUENT_KAFKA_BROKERS&apos;]&#125;&quot; default_topic &quot;#&#123;ENV[&apos;FLUENT_KAFKA_DEFAULT_TOPIC&apos;] || nil&#125;&quot; default_partition_key &quot;#&#123;ENV[&apos;FLUENT_KAFKA_DEFAULT_PARTITION_KEY&apos;] || nil&#125;&quot; default_message_key &quot;#&#123;ENV[&apos;FLUENT_KAFKA_DEFAULT_MESSAGE_KEY&apos;] || nil&#125;&quot; output_data_type &quot;#&#123;ENV[&apos;FLUENT_KAFKA_OUTPUT_DATA_TYPE&apos;] || &apos;json&apos;&#125;&quot; output_include_tag &quot;#&#123;ENV[&apos;FLUENT_KAFKA_OUTPUT_INCLUDE_TAG&apos;] || false&#125;&quot; output_include_time &quot;#&#123;ENV[&apos;FLUENT_KAFKA_OUTPUT_INCLUDE_TIME&apos;] || false&#125;&quot; exclude_topic_key &quot;#&#123;ENV[&apos;FLUENT_KAFKA_EXCLUDE_TOPIC_KEY&apos;] || false&#125;&quot; exclude_partition_key &quot;#&#123;ENV[&apos;FLUENT_KAFKA_EXCLUDE_PARTITION_KEY&apos;] || false&#125;&quot; get_kafka_client_log &quot;#&#123;ENV[&apos;FLUENT_KAFKA_GET_KAFKA_CLIENT_LOG&apos;] || false&#125;&quot; # ruby-kafka producer options max_send_retries &quot;#&#123;ENV[&apos;FLUENT_KAFKA_MAX_SEND_RETRIES&apos;] || 1&#125;&quot; required_acks &quot;#&#123;ENV[&apos;FLUENT_KAFKA_REQUIRED_ACKS&apos;] || -1&#125;&quot; ack_timeout &quot;#&#123;ENV[&apos;FLUENT_KAFKA_ACK_TIMEOUT&apos;] || nil&#125;&quot; compression_codec &quot;#&#123;ENV[&apos;FLUENT_KAFKA_COMPRESSION_CODEC&apos;] || nil&#125;&quot; max_send_limit_bytes &quot;#&#123;ENV[&apos;FLUENT_KAFKA_MAX_SEND_LIMIT_BYTES&apos;] || nil&#125;&quot; discard_kafka_delivery_failed &quot;#&#123;ENV[&apos;FLUENT_KAFKA_DISCARD_KAFKA_DELIVERY_FAILED&apos;] || false&#125;&quot;&lt;/match&gt; 后续可以把这些都作为环境变量从新构建镜像。(注意：没有注入默认值为nil的变量需要注释掉)(巨坑) k8s集群部署步骤–mongo 使用https://github.com/luolizhi/k8s_dockerfile/tree/master/fluentd-k8s-daemonset/v0.12/alpine-mongo文件下的Dockerfile构建fluentd-mongo-image镜像 修改上面目录下的镜像名称，修改环境变量mongo的参数值。","categories":[],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://luolizhi.github.io/tags/k8s/"},{"name":"fluentd","slug":"fluentd","permalink":"https://luolizhi.github.io/tags/fluentd/"},{"name":"kafka","slug":"kafka","permalink":"https://luolizhi.github.io/tags/kafka/"},{"name":"mongo","slug":"mongo","permalink":"https://luolizhi.github.io/tags/mongo/"}]},{"title":"ConfigMap","slug":"ConfigMap","date":"2018-03-22T09:19:12.000Z","updated":"2019-11-29T11:45:41.522Z","comments":true,"path":"2018/03/22/ConfigMap/","link":"","permalink":"https://luolizhi.github.io/2018/03/22/ConfigMap/","excerpt":"","text":"ConfigMap概览ConfigMap允许您将配置文件从容器镜像中解耦，从而增强容器应用的可移植性。 ConfigMap API资源保存配置数据的键值对，可以在pods中使用或者可以用于存储系统组件的配置数据。ConfigMap类似于Secrets,但是旨在更方便的使用不包含敏感信息的字符串。 注意：ConfigMap只引用属性文件，而不会替换它们。可以把ConfigMap联想成Linux中的/etc目录和它里面的内容。例如，假如您使用ConfigMap创建了Kubernetes Volume，ConfigMap中的每个数据项都代表该volume中的一个文件。 ConfigMap的data项中包含了配置数据。如下所示，可以是很简单的——如使用 —from-literal 参数定义的每个属性；也可以很复杂——如使用—from-file参数定义的配置文件或者json对象。 创建Configmap的四种方法 通过直接在命令行中指定configmap参数创建，即--from-literal 通过指定文件创建，即将一个配置文件创建为一个ConfigMap--from-file=&lt;文件&gt; 123&gt; 配置文件app.properties&gt; 创建命令（可以有多个--from-file）： &gt; ```kubectl create configmap test-config2 --from-file=./app.properties``` 通过指定目录创建，即将一个目录下的所有配置文件创建为一个ConfigMap，--from-file=&lt;目录&gt; 通过yaml文件来创建，另一种是通过kubectl直接在命令行下创建。事先写好标准的configmap的yaml文件，然后kubectl create -f 创建 Pod中使用ConfigMap的三种方法 第一种是通过环境变量的方式，直接传递给pod 使用configmap中指定的key 使用valueFrom、configMapKeyRef、name、key指定要用的key: 123456789101112131415161718192021apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: k8s.gcr.io/busybox command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot; ] env: - name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: name: special-config key: special.how - name: LOG_LEVEL valueFrom: configMapKeyRef: name: env-config key: log_level restartPolicy: Never 使用configmap中所有的key 还可以通过envFrom、configMapRef、name使得configmap中的所有key/value对都自动变成环境变量： 12345678910111213apiVersion: v1kind: Podmetadata: name: dapi-test-podspec: containers: - name: test-container image: k8s.gcr.io/busybox command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot; ] envFrom: - configMapRef: name: special-config restartPolicy: Never 第二种是通过在pod的命令行下运行的方式(启动命令中)在命令行下引用时，需要先设置为环境变量，之后可以通过$(VAR_NAME)设置容器启动命令的启动参数： 123456789101112131415161718192021apiVersion: v1kind: Podmetadata: name: dapi-test-podspec: containers: - name: test-container image: k8s.gcr.io/busybox command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)&quot; ] env: - name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: name: special-config key: SPECIAL_LEVEL - name: SPECIAL_TYPE_KEY valueFrom: configMapKeyRef: name: special-config key: SPECIAL_TYPE restartPolicy: Never 第三种是作为volume的方式挂载到pod内1234567891011121314151617181920212223apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx-configmapspec: replicas: 1 template: metadata: labels: app: nginx-configmap spec: containers: - name: nginx-configmap image: nginx ports: - containerPort: 80 volumeMounts: - name: config-volume4 mountPath: /tmp/config4 volumes: - name: config-volume4 configMap: name: test-config4 限制 我们必须在 Pod 使用 ConfigMap 之前，创建好 ConfigMap（除非您把 ConfigMap 标志成”optional”）。如果您引用了一个不存在的 ConfigMap， 那这个Pod是无法启动的。就像引用了不存在的 Key 会导致 Pod 无法启动一样。 如果您使用envFrom来从 ConfigMap 定义环境变量，无效的 Key 会被忽略。Pod可以启动，但是无效的名字将会被记录在事件日志里(InvalidVariableNames). 日志消息会列出来每个被忽略的 Key ，比如： 123kubectl get eventsLASTSEEN FIRSTSEEN COUNT NAME KIND SUBOBJECT TYPE REASON SOURCE MESSAGE0s 0s 1 dapi-test-pod Pod Warning InvalidEnvironmentVariableNames &#123;kubelet, 127.0.0.1&#125; Keys [1badkey, 2alsobad] from the EnvFrom configMap default/myconfig were skipped since they are considered invalid environment variable names. ConfigMaps 存在于指定的 命名空间.则这个 ConfigMap 只能被同一个命名空间里的 Pod 所引用。 Kubelet 不支持在API服务里找不到的Pod使用ConfigMap，这个包括了每个通过 Kubectl 或者间接通过复制控制器创建的 Pod， 不包括通过Kubelet 的 –manifest-url 标志, –config 标志, 或者 Kubelet 的 REST API。（注意：这些并不是常规创建 Pod 的方法）configmap的热更新研究更新 ConfigMap 后: 使用该 ConfigMap 挂载的 Env 不会同步更新 使用该 ConfigMap 挂载的 Volume 中的数据需要一段时间（实测大概10秒）才能同步更新ENV 是在容器启动的时候注入的，启动之后 kubernetes 就不会再改变环境变量的值，且同一个 namespace 中的 pod 的环境变量是不断累加的，参考 Kubernetes中的服务发现与docker容器间的环境变量传递源码探究。为了更新容器中使用 ConfigMap 挂载的配置，可以通过滚动更新 pod 的方式来强制重新挂载 ConfigMap，也可以在更新了 ConfigMap 后，先将副本数设置为 0，然后再扩容。 ps 一个demo先创建一个如下所示的配置文件。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152apiVersion: v1data: DASHBOARD.CONF.INI: | [mysqld] log-bin = mysql-bin [port] serviceport=&quot;80&quot; IMAGE_VERSION: v2.0 OTHERKEY: OTHERVALUE REPLICAS: &quot;2&quot;kind: ConfigMapmetadata: name: tst-config namespace: default``` 然后将对应的key设置成容器的环境变量。 ```$xsltapiVersion: v1kind: Podmetadata: name: test-podspec: containers: - name: test-container image: busybox:latest command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot; ] env: - name: REPLICAS valueFrom: configMapKeyRef: name: tst-config key: REPLICAS - name: OTHERKEY valueFrom: configMapKeyRef: name: tst-config key: OTHERKEY - name: IMAGE_VERSION valueFrom: configMapKeyRef: name: tst-config key: IMAGE_VERSION restartPolicy: Never``` 当Pod结束后会输出```$xsltREPLICAS=2IMAGE_VERSION=v2.0OTHERKEY=OTHERVALUE","categories":[],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://luolizhi.github.io/tags/k8s/"},{"name":"configmap","slug":"configmap","permalink":"https://luolizhi.github.io/tags/configmap/"}]},{"title":"kubernetes 常用命令","slug":"kubernetes-常用命令","date":"2018-03-22T09:14:33.000Z","updated":"2019-11-29T11:45:41.528Z","comments":true,"path":"2018/03/22/kubernetes-常用命令/","link":"","permalink":"https://luolizhi.github.io/2018/03/22/kubernetes-常用命令/","excerpt":"","text":"kubectl get pods -o wide使用nslookup查看这些Pod的DNSkubectl run -i --tty --image busybox dns-test --restart123/ # nslookup web-0.nginxServer: 10.0.0.10Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local 用kubectl run来创建一个CronJob：1kubectl run hello --schedule=\"*/1 * * * *\" --restart=OnFailure --image=busybox -- /bin/sh -c \"date; echo Hello from the Kubernetes cluster\" 三种容器类型 静态容器组：bare pod, IP和Hostname恒定； 有状态伸缩组： Deployment, IP和Hostname恒定，支持IP池； 无状态伸缩组： StatefulSets, 支持IP池，IP随机分配，灵活度更高。 斜体黑体 k8s容器命名规则","categories":[],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://luolizhi.github.io/tags/kubernetes/"}]}]}